d = [{'timestamp': (0.0, 5.36), 'text': ' Hello guys, welcome to my new coding video. In this video we will be coding Lama2 from scratch.'}, {'timestamp': (6.08, 11.36), 'text': ' Just like the previous video in which I was coding the transformer model from zero,'}, {'timestamp': (12.16, 19.57), 'text': ' while coding I will also explain all the aspects of Lama. So all the each building block of the Lama architecture.'}, {'timestamp': (19.57, 24.93), 'text': ' And I will also explain the math behind the rotary positional encoding. I will also explain'}, {'timestamp': (24.93, 33.45), 'text': ' grouped query attention, the KV cache. So we will not only have a theoretical view of these concepts but also a practical one.'}, {'timestamp': (33.45, 37.19), 'text': ' If you are not familiar with the transformer model I highly recommend you watch my previous'}, {'timestamp': (37.19, 41.43), 'text': ' video on the transformer model and then if you want you can also watch the previous video'}, {'timestamp': (41.43, 43.76), 'text': ' on how to code a transformer model from 0, because'}, {'timestamp': (43.76, 49.74), 'text': ' it will really help you a lot in understanding what we are doing in this current video.'}, {'timestamp': (49.74, 54.54), 'text': ' If you already watched my previous video on the architecture of Lama in which I explained'}, {'timestamp': (54.54, 57.47), 'text': " the concept, it will be also really helpful if you didn't"}, {'timestamp': (57.47, 64.75), 'text': " it's I will try to explain all the concept but not as much in the as in"}, {'timestamp': (64.75, 69.35), 'text': ' the in detail as in the last video so please if you have time if you want to'}, {'timestamp': (69.35, 72.59), 'text': ' please watch my previous video on the lama architecture and then watch this video'}, {'timestamp': (72.59, 76.07), 'text': ' because this will really give you a deeper understanding of what is happening and'}, {'timestamp': (77.99, 81.61), 'text': " Let's review lama the lama architecture here"}, {'timestamp': (81.61, 85.04), 'text': ' We have a comparison between the architecture of the standard transformer'}, {'timestamp': (85.04, 90.24), 'text': ' as introduced in the attention is all you need paper and the architecture of Lama.'}, {'timestamp': (90.24, 99.77), 'text': ' The first thing we notice is that the transformer was an encoder decoder model and in the previous video we actually trained it on a translation'}, {'timestamp': (99.77, 105.61), 'text': ' task on how to translate for example from English to Italian while Lama is a large language'}, {'timestamp': (105.61, 113.41), 'text': ' model so the goal of a large language model is actually to work with the what is called the next token prediction task'}, {'timestamp': (113.45, 117.73), 'text': ' So given a prompt the model tries to come up with the next token that'}, {'timestamp': (118.81, 126.34), 'text': ' Complets this prompt in the most coherent way. So in the in a way that it makes sense the answer. And we keep'}, {'timestamp': (126.34, 132.26), 'text': ' asking the model for the successive tokens based on the previous tokens. So'}, {'timestamp': (132.26, 136.58), 'text': " this is why it's called a causal model. So each output depends on the previous"}, {'timestamp': (136.58, 137.05), 'text': " tokens which is also called the prompt. it's called a causal model. So each output depends on the previous tokens,"}, {'timestamp': (137.05, 138.77), 'text': ' which is also called the prompt.'}, {'timestamp': (140.97, 142.69), 'text': ' Contrary to what I have done to,'}, {'timestamp': (142.69, 145.97), 'text': ' in my previous video on coding the transformer model,'}, {'timestamp': (145.97, 148.49), 'text': ' in this video, we will not start by coding'}, {'timestamp': (148.49, 152.91), 'text': ' the single building blocks of Lama and then come up with a bigger'}, {'timestamp': (152.91, 158.67), 'text': ' picture, but we will start from the bigger picture, so we will first make the skeleton'}, {'timestamp': (158.67, 162.31), 'text': ' of the architecture and then we will build each block.'}, {'timestamp': (162.31, 165.84), 'text': ' I find that this is a better way for explaining Lama,'}, {'timestamp': (165.84, 171.6), 'text': ' also because the model is simpler, even if the single building blocks are much more complex.'}, {'timestamp': (171.6, 176.24), 'text': " And this is why it's better to first look at how they interact with each other and then"}, {'timestamp': (176.24, 180.01), 'text': ' zoom in into their inner workings.'}, {'timestamp': (180.01, 183.37), 'text': " Let's start our journey with the embeddings."}, {'timestamp': (183.37, 186.41), 'text': ' So this block here, let me use the laser.'}, {'timestamp': (186.41, 191.79), 'text': ' So this block here, so we are given an input and we want to convert it into embeddings.'}, {'timestamp': (192.67, 198.43), 'text': " Let's also review what are embeddings. These are my slides from my previous video on the"}, {'timestamp': (198.43, 203.44), 'text': ' transformer and as you can see we start with an input sentence.'}, {'timestamp': (203.44, 206.8), 'text': ' So we are given for example the sentence your cat is a lovely cat.'}, {'timestamp': (206.8, 210.48), 'text': ' We tokenize it so we split into single tokens.'}, {'timestamp': (210.48, 214.0), 'text': ' We map each token into its position in the vocabulary.'}, {'timestamp': (214.0, 219.99), 'text': ' The vocabulary is the list of all the words that our model can recognize.'}, {'timestamp': (219.99, 224.49), 'text': ' These tokens, actually, most of the time are not single words.'}, {'timestamp': (224.49, 229.43), 'text': " What I mean is that the model doesn't just split the word by white space into single"}, {'timestamp': (229.43, 230.75), 'text': ' words.'}, {'timestamp': (230.75, 235.83), 'text': ' Usually the most commonly used tokenizer is the BPE tokenizer, which means byte pair encoding'}, {'timestamp': (235.83, 243.11), 'text': ' tokenizer, in which the single tokens can also be a sequence of letters that not necessarily'}, {'timestamp': (243.11, 254.24), 'text': ' map to a single word. It may be a part of letters that not necessarily map to a single word.'}, {'timestamp': (254.24, 260.11), 'text': ' The embedding is a mapping between the number, so the input IDs that represent the'}, {'timestamp': (260.11, 264.63), 'text': ' position of the token inside of the vocabulary to a vector.'}, {'timestamp': (264.63, 276.15), 'text': " This vector in the original transformer was of size 512, while in Lama, in the the base model so the 7 billion model it's 4,096"}, {'timestamp': (276.15, 282.55), 'text': ' the dimension is 4,096 which means this vector will contains 4,096'}, {'timestamp': (282.55, 288.32), 'text': ' numbers each of these vectors represents somehow the meaning of the word,'}, {'timestamp': (288.32, 293.92), 'text': ' because each of these vectors are actually parameter vectors that are trained along the'}, {'timestamp': (293.92, 297.85), 'text': ' model and somehow they capture the meaning of each word.'}, {'timestamp': (297.85, 303.73), 'text': ' So for example if we take the word cat and dog they will have an embedding that is more'}, {'timestamp': (303.73, 314.67), 'text': ' similar compared to cat and tree if we compare the distance of these two vectors so we could say the Euclidean distance of these two vectors.'}, {'timestamp': (314.67, 321.67), 'text': ' And this is why they are called embedding, this is why we say they capture the meaning of the word.'}, {'timestamp': (321.67, 324.8), 'text': " So let's start coding the model."}, {'timestamp': (324.8, 329.84), 'text': ' We open, I will open Visual Studio Code.'}, {'timestamp': (329.84, 332.92), 'text': ' First of all, we make sure that we have the necessary libraries.'}, {'timestamp': (332.92, 336.52), 'text': ' I will also share the repository of this project.'}, {'timestamp': (336.52, 338.45), 'text': ' The only thing we need is Torch, CentesPiece'}, {'timestamp': (338.45, 343.37), 'text': ' which is the tokenizer that we use in Lama and TKDM.'}, {'timestamp': (343.37, 346.97), 'text': ' The second thing is from the official repository of Lama, you should download the download'}, {'timestamp': (346.97, 351.59), 'text': " script, it's this file download.sh that allows you to download the weights of"}, {'timestamp': (351.59, 352.59), 'text': ' the Lama model.'}, {'timestamp': (352.59, 358.35), 'text': ' In my case, I have downloaded the Lama 2.7 billion along with the tokenizer.'}, {'timestamp': (358.35, 361.51), 'text': ' And this is the smallest model actually.'}, {'timestamp': (361.51, 366.4), 'text': ' And I will not even be able to use the GPU because my GPU is not powerful enough.'}, {'timestamp': (366.4, 370.6), 'text': ' And so I will run the model on the CPU. I think most of you guys will do the same because'}, {'timestamp': (370.6, 381.89), 'text': " it's actually a little bit for a normal computer unless you have a powerful GPU. So let's start coding it. Let's create a new file, model.py"}, {'timestamp': (382.61, 420.65), 'text': " and let's start our journey. Import the necessary things. Okay, these are the basic things that we always import. I remember we also need math and then also data classes. And this is all the imports we need."}, {'timestamp': (420.65, 424.61), 'text': " Most of the code is actually based on the original Lama code, so you don't be surprised"}, {'timestamp': (424.61, 429.49), 'text': " if you see a lot of similarities, but I simplified a lot of parts to remove things that we don't"}, {'timestamp': (429.49, 431.95), 'text': " need, especially for example the parallelization. And I did a lot of parts to remove things that we don't need, especially for example, the parallelization."}, {'timestamp': (434.45, 440.19), 'text': ' I also tried to add a lot of comments to show you all the shapes changed in each tensor.'}, {'timestamp': (440.71, 442.87), 'text': " And that's it. So let's start."}, {'timestamp': (442.87, 446.18), 'text': ' So the first thing I want to create is the class'}, {'timestamp': (446.18, 480.47), 'text': ' that represents the parameters of the model. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� Here we already see that we have two types of heads.'}, {'timestamp': (480.47, 489.92), 'text': ' One is the number of heads for the queries, so number of heads for the queries and here we have the number of heads for the'}, {'timestamp': (489.92, 498.47), 'text': " K and the V, so the keys and the values because we will see later with grouped query attention we don't have to necessarily"}, {'timestamp': (498.47, 503.05), 'text': ' have the same number of heads for the query key and values like in the original transformer,'}, {'timestamp': (503.05, 505.87), 'text': ' but we can have multiple number of heads.'}, {'timestamp': (505.87, 516.19), 'text': ' And we will see why and how they work.'}, {'timestamp': (549.58, 552.19), 'text': ' This will be set when we load the tokenizer. Thank you. These two parameters indicate the hidden dimension of the FFN layer, so the field forward layer. The basic idea is that they try to, when they'}, {'timestamp': (552.19, 556.51), 'text': ' introduce the grouped query attention, they try to keep the number of parameters because'}, {'timestamp': (556.51, 560.51), 'text': ' with the grouped query attention we reduce the number of heads of the K and V, but they'}, {'timestamp': (560.51, 563.36), 'text': ' incremented the number of parameters of the feedforward layer.'}, {'timestamp': (563.44, 569.96), 'text': ' So as the number of the total parameters of the model remains the same this allows to compare the full'}, {'timestamp': (570.6, 578.54), 'text': ' the base transformer so with all the heads for the query the key and values with the one they use in Lama which has a reduced'}, {'timestamp': (578.54, 585.26), 'text': ' number of heads for the KNB but this is just a decision an architectural decision then we have some'}, {'timestamp': (585.9, 601.87), 'text': ' EPS this is a number that is very small and we will see why we need it.'}, {'timestamp': (601.87, 615.16), 'text': ' My god. And these are all the parameters we need.'}, {'timestamp': (615.16, 618.62), 'text': ' Also here we have two parameters that we will later use for the KV cache.'}, {'timestamp': (618.62, 621.98), 'text': ' And I will explain you later what is it and how it works.'}, {'timestamp': (621.98, 626.5), 'text': " Let's start as I said before, let's start with implementing the skeleton of the entire"}, {'timestamp': (626.5, 629.78), 'text': ' module and then we implement each single part.'}, {'timestamp': (629.78, 634.39), 'text': ' And while implementing each single part, I will also review the background and how it'}, {'timestamp': (634.39, 641.99), 'text': ' works and the maths behind it.'}, {'timestamp': (641.99, 646.4), 'text': ' This is the main class that will represent the entire model so all the model we can see'}, {'timestamp': (646.4, 648.6), 'text': ' here.'}, {'timestamp': (648.6, 702.14), 'text': ' So all this model here except for the Softmax. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� We make sure that we have set the vocabulary size. Save the model.'}, {'timestamp': (702.14, 709.7), 'text': ' So this is represents this block here is repeated many times one after another just like in the'}, {'timestamp': (709.7, 711.39), 'text': ' transformer the base transformer,'}, {'timestamp': (711.39, 715.63), 'text': ' if you remember correctly, this block here and this block here were repeated one after another'}, {'timestamp': (715.63, 722.83), 'text': " many times, and here it's repeated 32 times, and the output of the last layer is then sent to"}, {'timestamp': (722.83, 749.32), 'text': " this RMS norm, then to the linear, etc. I'm using the same names as used in the code from the Lama repository because when we will"}, {'timestamp': (749.32, 751.57), 'text': ' load the weights of the model the'}, {'timestamp': (751.57, 756.29), 'text': " names must match otherwise the torch doesn't know where to load the weights in"}, {'timestamp': (756.29, 761.23), 'text': " and this is the reason I'm trying to keep the same name I only changed some"}, {'timestamp': (761.23, 798.11), 'text': ' names to make them more clear but most of the other list of the layers. We will create later the encoder block, which is each of these blocks here.'}, {'timestamp': (798.11, 800.35), 'text': ' This is the encoder block.'}, {'timestamp': (800.35, 802.07), 'text': ' For now we just create the skeleton.'}, {'timestamp': (802.07, 804.8), 'text': ' So we have a list of these blocks, then we have a normalization'}, {'timestamp': (807.28, 810.96), 'text': ' and the normalization is the RMS normalization, we will implement it later.'}, {'timestamp': (812.24, 817.9), 'text': ' We need to tell him the size of the features and the EPS is a very small number'}, {'timestamp': (817.9, 821.34), 'text': ' that is needed for the normalization calculation'}, {'timestamp': (821.34, 824.06), 'text': ' so that we never divide it by zero, basically.'}, {'timestamp': (829.1, 833.67), 'text': ' And then we have the output layer.'}, {'timestamp': (833.67, 849.52), 'text': ' ok then we need to pre-compute the frequencies of the rotary positional encodings.'}, {'timestamp': (849.52, 855.4), 'text': " So let's do it."}, {'timestamp': (855.4, 901.14), 'text': " I created this method and then we go to implement it and I will show you how it works. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� I'm checking, I think we have a parenthesis. Okay. This is the base transformer model."}, {'timestamp': (901.14, 903.66), 'text': ' So first of all we have n layers.'}, {'timestamp': (903.66, 906.18), 'text': ' We have first of all the input embeddings.'}, {'timestamp': (906.18, 912.15), 'text': ' So we convert the input into embeddings. Then we pass it through a list of layers.'}, {'timestamp': (912.15, 916.11), 'text': ' The last layer output is sent to the normalization, then to the output.'}, {'timestamp': (916.11, 938.52), 'text': ' So this logic will be more clear in the forward method. Ok, here you will see one thing that is different from the previous transformers is that the'}, {'timestamp': (938.52, 941.84), 'text': ' sequence length that we want is always 1.'}, {'timestamp': (941.84, 945.28), 'text': ' And this is because we are using the KV cache.'}, {'timestamp': (945.28, 946.76), 'text': ' And we will see why.'}, {'timestamp': (946.76, 951.67), 'text': " So in the previous, let's review here, here."}, {'timestamp': (952.61, 955.89), 'text': ' When we give the input, we want to give the prompt'}, {'timestamp': (955.89, 959.93), 'text': ' and the model will give us a softmax of the next token.'}, {'timestamp': (959.93, 962.05), 'text': " But with the KVcache, we don't need to give"}, {'timestamp': (962.05, 964.64), 'text': ' all the previous tokens because maybe we already'}, {'timestamp': (964.64, 969.36), 'text': ' computed them in the previous iteration so we only need to give the latest token and'}, {'timestamp': (969.36, 971.72), 'text': ' then the model will put the next token.'}, {'timestamp': (971.72, 975.56), 'text': ' While the intermediate tokens, so the cache of the previous tokens will be kept by the'}, {'timestamp': (975.56, 977.5), 'text': ' model in its cache because here we have a'}, {'timestamp': (977.5, 981.98), 'text': ' kvcache but we will see this mechanism later so for now just remember that the input here'}, {'timestamp': (981.98, 984.7), 'text': ' that we will get is one token at a time.'}, {'timestamp': (989.9, 1007.0), 'text': ' So we will get a batch with sequence length. And we make sure that the sequence length is actually 1.'}, {'timestamp': (1015.84, 1019.46), 'text': ' And second thing is this model is only good for inferencing, not for training, because'}, {'timestamp': (1019.46, 1023.98), 'text': ' for training of course we need to not have the KB cache and we only need to be able to'}, {'timestamp': (1023.98, 1026.38), 'text': ' process multiple tokens.'}, {'timestamp': (1026.38, 1042.07), 'text': ' But our goal is actually to use the pre-trained Lama weights.'}, {'timestamp': (1042.07, 1048.04), 'text': ' So we convert the tokens into embeddings.'}, {'timestamp': (1048.04, 1055.64), 'text': ' As you can see we add the dimension of the embeddings which is 4096 for the base model'}, {'timestamp': (1055.64, 1096.0), 'text': ' but depending on the model size it can be different. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� Okay, I promise I will explain this line here and this line here much more in detail just'}, {'timestamp': (1096.0, 1098.46), 'text': ' in two minutes. Let me finish it and I will explain'}, {'timestamp': (1098.46, 1103.9), 'text': ' everything together. So this is basically we are we pre-compute something about the positional'}, {'timestamp': (1103.9, 1113.55), 'text': " encoding that we then give to the successive layers. But let's finish writing it and then we expand this method and this one."}, {'timestamp': (1113.55, 1118.59), 'text': ' And everything will be clear. So suppose we with this one we retrieve'}, {'timestamp': (1118.59, 1122.11), 'text': ' something that is needed for computing the positional encoding which then we'}, {'timestamp': (1122.11, 1146.84), 'text': ' feed to the next layers. So we consecutively apply all the encoder blocks and finally we apply the normalization'}, {'timestamp': (1146.84, 1153.35), 'text': ' just like here. So we apply these blocks one after another many times, then we'}, {'timestamp': (1153.35, 1158.27), 'text': ' apply the normalization and then we calculate the output using the linear'}, {'timestamp': (1158.27, 1164.3), 'text': ' layer. And finally we return the output.'}, {'timestamp': (1164.3, 1166.92), 'text': ' So this is the skeleton of the model.'}, {'timestamp': (1166.92, 1170.62), 'text': ' So we take the input, we convert it into embeddings.'}, {'timestamp': (1170.62, 1172.08), 'text': ' This part I will explain later.'}, {'timestamp': (1172.08, 1178.38), 'text': ' We give these input embeddings with something about the positional encodings to these blocks, one after another.'}, {'timestamp': (1178.38, 1183.1), 'text': ' We take the output of the last one, give it to the RMS norm, we take the output of the RMS norm,'}, {'timestamp': (1183.1, 1187.74), 'text': ' give it to the linear layer and then during the inference we will apply the softmax.'}, {'timestamp': (1188.62, 1191.95), 'text': " Now, let's concentrate on the positional encodings."}, {'timestamp': (1191.95, 1195.63), 'text': " Let's first review how they worked in the original transformer."}, {'timestamp': (1195.63, 1197.67), 'text': ' As you remember, in the original transformer,'}, {'timestamp': (1197.67, 1200.99), 'text': ' so the transformer in the attention is all you need,'}, {'timestamp': (1200.99, 1205.08), 'text': " we first take the sentence, we convert into embedding vectors so if it's a"}, {'timestamp': (1205.08, 1211.2), 'text': ' vectors of size 512 we then add another vector which has the same size so 512'}, {'timestamp': (1211.2, 1216.52), 'text': ' that represents the position of that token inside the sentence so every'}, {'timestamp': (1216.52, 1233.71), 'text': ' sentence every token in the first position of a sentence از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از از this vector added to it and every token in the third position of a sentence will have this vector'}, {'timestamp': (1233.71, 1240.59), 'text': ' added to it. These vectors are pre-computed because they only depend on the position not on the word'}, {'timestamp': (1240.59, 1244.88), 'text': ' they are applied to and this is why they are called absolute positional encoding because'}, {'timestamp': (1244.88, 1251.36), 'text': ' they only strictly depend on the position of the word inside of the sentence. While in the'}, {'timestamp': (1251.36, 1255.44), 'text': " contrary in the rotary positional embeddings they are a little different, let's go have a look."}, {'timestamp': (1255.44, 1258.22), 'text': " First of all the rotary positional encodings or embeddings they are a little different. Let's go have a look. First of all, the rotary positional encodings"}, {'timestamp': (1258.22, 1264.14), 'text': ' or embeddings, they are computed right before the calculation of the attention and they are only'}, {'timestamp': (1264.14, 1270.99), 'text': " applied to the Q and the K matrices, not to the V. Let's see why."}, {'timestamp': (1270.99, 1274.41), 'text': ' The first thing we need to understand is the difference between absolute positional encodings'}, {'timestamp': (1274.41, 1275.57), 'text': ' and relative ones.'}, {'timestamp': (1275.57, 1279.81), 'text': ' The absolute positional encodings are the ones we just saw, so they deal with one token'}, {'timestamp': (1279.81, 1283.05), 'text': ' at a time and each token gets its own embedding.'}, {'timestamp': (1283.05, 1284.96), 'text': ' While the relative positional embeddings,'}, {'timestamp': (1284.96, 1289.68), 'text': ' they come into play during the calculation of the attention. And the calculation of the'}, {'timestamp': (1289.68, 1295.56), 'text': " attention is basically done using the dot product, because it's query multiplied by"}, {'timestamp': (1295.56, 1298.54), 'text': ' the transpose of the keys divided by the square root of the model.'}, {'timestamp': (1298.54, 1303.42), 'text': ' So there is a dot product in between and that we want with the relative positional encodings.'}, {'timestamp': (1303.42, 1309.26), 'text': ' We change this dot product so that we introduce a new vector that indicates the distance between'}, {'timestamp': (1309.26, 1312.47), 'text': ' the two tokens involved in the dot product.'}, {'timestamp': (1312.47, 1318.97), 'text': ' So for example, in the original transformer, we have this formula here, so query multiplied'}, {'timestamp': (1318.97, 1324.88), 'text': ' by the transport of the keys divided by the square root of the model, while in the relative positional encodings,'}, {'timestamp': (1324.88, 1326.88), 'text': ' which are not the one used in Lama.'}, {'timestamp': (1326.88, 1329.56), 'text': ' So, this is just an introduction.'}, {'timestamp': (1329.56, 1333.44), 'text': ' In the relative positional encodings, we have another vector here that represents the'}, {'timestamp': (1333.44, 1339.26), 'text': ' distance between these two tokens, and we compute the attention mechanism like this.'}, {'timestamp': (1339.26, 1344.54), 'text': ' The rotary positional embeddings, the ones that are used in Lama, are something in between'}, {'timestamp': (1344.54, 1347.38), 'text': ' the absolute and the relative.'}, {'timestamp': (1347.38, 1354.71), 'text': ' Absolute because each token will get its own embedding but relative because the attention'}, {'timestamp': (1354.71, 1359.91), 'text': ' mechanism will be evaluated using the relative distance between two tokens.'}, {'timestamp': (1359.91, 1362.23), 'text': " Let's see."}, {'timestamp': (1362.23, 1368.96), 'text': ' The rotary position embeddings were introduced in the paper from'}, {'timestamp': (1368.96, 1379.66), 'text': ' this company, J.E. and the authors of this paper they wanted to find an inner product that works like this.'}, {'timestamp': (1379.66, 1381.74), 'text': ' So first of all, what is an inner product?'}, {'timestamp': (1381.74, 1384.7), 'text': ' We are all familiar with the dot product.'}, {'timestamp': (1384.7, 1388.94), 'text': ' The inner product can be thought of as a generalization of the dot product.'}, {'timestamp': (1388.94, 1392.67), 'text': " So it's an operation that has some properties"}, {'timestamp': (1392.67, 1398.99), 'text': ' that reflect what is the dot product. So the authors of this paper wanted to find an inner'}, {'timestamp': (1398.99, 1408.6), 'text': ' product between the two vectors query and key such that they only depend on the so this is the'}, {'timestamp': (1408.6, 1413.08), 'text': ' symbol for inner product so this inner product only depends on the embedding of'}, {'timestamp': (1413.08, 1420.34), 'text': ' the two tokens involved so xm and xn and the relative distance of these two tokens involved, so xm and xn, and the relative distance of these two tokens, so'}, {'timestamp': (1420.34, 1422.62), 'text': ' the distance between them.'}, {'timestamp': (1422.62, 1429.5), 'text': ' For example if the first token is in position 2 and the second token is in position 5, so'}, {'timestamp': (1429.5, 1433.11), 'text': ' m equal 2, n equal 5, the distance between 2 will'}, {'timestamp': (1433.11, 1438.35), 'text': ' be 3 or minus 3 according to the order.'}, {'timestamp': (1438.35, 1442.91), 'text': ' And they wanted to find a dot product that has this property that only depends on the'}, {'timestamp': (1442.91, 1445.04), 'text': ' embedding of the first token, on the embedding of the second token, and the dot product that has this property that only depends on the embedding of the first token,'}, {'timestamp': (1445.04, 1449.64), 'text': ' on the embedding of the second token and the relative distance between them.'}, {'timestamp': (1449.64, 1459.42), 'text': ' Then they show that if this function g is built in this way then we achieve that objective. That is we take the first token, so the query'}, {'timestamp': (1459.42, 1464.62), 'text': ' for example. We multiply it by the W matrix, this is actually done also in the Vanilla'}, {'timestamp': (1464.62, 1475.59), 'text': ' transformer but ok, suppose there is no W matrix here. We convert it into a complex number in this form. We take the key vector,'}, {'timestamp': (1475.59, 1481.75), 'text': ' we transform into a complex number into this form and we define the inner product in this'}, {'timestamp': (1481.75, 1487.0), 'text': ' way. This inner product will basically depend only on the'}, {'timestamp': (1487.0, 1489.72), 'text': ' distance between these two tokens.'}, {'timestamp': (1489.72, 1494.9), 'text': ' So they wanted to find an encoding mechanism such that the attention mechanism which is'}, {'timestamp': (1494.9, 1499.18), 'text': ' based on a dot product which is an inner product behaves'}, {'timestamp': (1499.18, 1500.18), 'text': ' like this.'}, {'timestamp': (1500.18, 1505.22), 'text': ' So it only depends on the embeddings of the vector and the distance between them.'}, {'timestamp': (1505.22, 1513.55), 'text': ' And if we, for example, this formulation here, if we apply it on a vector of dimension 2 so with embedding of'}, {'timestamp': (1513.55, 1520.35), 'text': ' with only two dimensions it becomes in this form here. This is due to the Ehlers formula so each'}, {'timestamp': (1520.35, 1528.16), 'text': ' complex number thanks to the Ehlers formula can be written as the cosine plus a sine'}, {'timestamp': (1529.6, 1539.58), 'text': ' and this matrix here reminds us of the rotation matrix. Let me give you an example. Suppose our original vector is here'}, {'timestamp': (1540.38, 1549.58), 'text': ' and if we multiply this vector v0 by this matrix here, the resulting vector will be rotated by'}, {'timestamp': (1549.58, 1551.19), 'text': ' the angle theta.'}, {'timestamp': (1551.19, 1553.79), 'text': " So this is why they're called rotary positional embeddings"}, {'timestamp': (1553.79, 1558.47), 'text': ' because the matrix here represents a rotation of the vector.'}, {'timestamp': (1558.47, 1561.59), 'text': ' So when we want to visualize'}, {'timestamp': (1561.59, 1564.96), 'text': ' how the rotary positional embedding work, we have to think that they will map'}, {'timestamp': (1564.96, 1572.0), 'text': ' it into a vector space and they will rotate each word to an angle that is a multiple of'}, {'timestamp': (1572.0, 1579.14), 'text': ' a base angle, so theta, and proportional to the theta angle proportional according to its position'}, {'timestamp': (1579.14, 1585.62), 'text': ' so that two tokens that occupy similar positions will have similar inclination'}, {'timestamp': (1585.62, 1591.47), 'text': ' and the two tokens have different positions will have different inclinations.'}, {'timestamp': (1591.47, 1594.43), 'text': ' And this is the idea behind the rotary positional embeddings.'}, {'timestamp': (1594.43, 1600.31), 'text': ' But how do we actually compute them in the code in the PyTorch?'}, {'timestamp': (1600.31, 1604.93), 'text': ' Well to compute them we need to build a matrix like this.'}, {'timestamp': (1604.93, 1609.49), 'text': ' And actually as you can see this matrix is actually full of zeros.'}, {'timestamp': (1609.49, 1614.45), 'text': ' And so when we calculate the embedding in this way we will do, if we do a matrix multiplication'}, {'timestamp': (1614.45, 1617.5), 'text': ' we will be doing a lot of operations that are useless because'}, {'timestamp': (1617.5, 1620.02), 'text': ' most of these items are zero.'}, {'timestamp': (1620.02, 1625.34), 'text': ' So the authors of the paper proposed another form that is more computationally efficient.'}, {'timestamp': (1625.34, 1632.11), 'text': ' And this form basically says that we take the embedding of the vector to which we want to apply the positional encodings.'}, {'timestamp': (1632.11, 1634.63), 'text': ' So for example this one, this is the vector.'}, {'timestamp': (1634.63, 1638.71), 'text': ' So the first dimension, the second dimension, the third dimension and the last dimension.'}, {'timestamp': (1638.71, 1648.09), 'text': ' So if this is for example the vanilla transformer, this would be xd, which should be 512. We multiply it element wise by this'}, {'timestamp': (1648.09, 1655.45), 'text': ' matrix plus another vector that is actually based on this vector but with the positions'}, {'timestamp': (1655.45, 1657.18), 'text': ' and the signs changed.'}, {'timestamp': (1657.18, 1661.02), 'text': ' So this is actually in the first position we have the second dimension with its sign'}, {'timestamp': (1661.02, 1662.02), 'text': ' changed.'}, {'timestamp': (1662.02, 1664.7), 'text': ' In the second position we have actually the first dimension.'}, {'timestamp': (1664.7, 1668.66), 'text': ' In the third position we have the fourth dimension but with sign changed.'}, {'timestamp': (1668.66, 1672.03), 'text': ' So it depends only on the embedding of the word but'}, {'timestamp': (1673.71, 1679.31), 'text': ' changes with the signs and position change and then we multiply this element wise with another'}, {'timestamp': (1680.35, 1685.89), 'text': ' matrix that you can see here this vector then this will be the encoding of the'}, {'timestamp': (1685.89, 1693.65), 'text': ' token we are talking about and now what we can pre-compute is this matrix here'}, {'timestamp': (1693.65, 1698.22), 'text': " because it doesn't depend on the token to which we apply it to and this matrix here because it doesn't depend on the token to which we apply it to and this matrix here"}, {'timestamp': (1698.22, 1704.62), 'text': " because it doesn't depend on the token we apply it to and they depend on M so it's the position"}, {'timestamp': (1704.62, 1712.67), 'text': ' of the word and theta what is theta theta is a series of numbers defined like this.'}, {'timestamp': (1712.67, 1720.15), 'text': " And so let's first build the code to pre-compute this and this here."}, {'timestamp': (1720.15, 1721.15), 'text': " Let's do it."}, {'timestamp': (1721.15, 1755.63), 'text': ' I will first write the code and later I will show you how it works. This data parameter takes,000 comes from the paper.'}, {'timestamp': (1755.63, 1762.23), 'text': " It's written here, 10,000."}, {'timestamp': (1762.23, 1766.77), 'text': ' We first need to make sure that the dimension of the word to which we are applying the embedding'}, {'timestamp': (1766.77, 1772.73), 'text': " is actually even because in the paper it's written that this rotary positional encodes"}, {'timestamp': (1772.73, 1782.9), 'text': ' cannot be applied to an embedding which has odd dimensions dimension so cannot be 513 must be 512 or 514 or any'}, {'timestamp': (1782.9, 1803.31), 'text': ' other event number. And this is as written in the paper.'}, {'timestamp': (1803.31, 1824.46), 'text': ' Now we build the CTA parameters which is a sequence. And the shape of this data will be head dimension divided by 2.'}, {'timestamp': (1824.46, 1833.15), 'text': ' Because we will apply these embeddings to each head, so not right after the embedding, but after we have split them into multi-head,'}, {'timestamp': (1833.15, 1840.91), 'text': ' so each token, the token of each head, we check the size of the dimension of each head'}, {'timestamp': (1840.91, 1846.01), 'text': ' and we divide it by 2, because, why divide by 2? Because in the paper, they we divide it by 2 because in the paper they also divide it'}, {'timestamp': (1846.01, 1847.01), 'text': ' by 2.'}, {'timestamp': (1847.01, 1862.02), 'text': " So D divided by 2 here. Okay, so what's the formula here?"}, {'timestamp': (1862.02, 1876.75), 'text': ' The formula is theta of i is equal to 10,000 to the power of minus 2 multiplied by i minus 1 divided by dimension 4i'}, {'timestamp': (1877.63, 1889.73), 'text': ' equal to 1 2 etc up to dimension divided by 2. So now we are computing this part here. So this part here'}, {'timestamp': (1889.73, 1895.37), 'text': " which is a series. So I here it starts from 1 we will start from 0 so we don't"}, {'timestamp': (1895.37, 1898.5), 'text': ' have to do i-1.'}, {'timestamp': (1898.5, 1916.31), 'text': ' And theta is equal to 1 over the theta, so 10,000 to the power of theta numerator divided by the dimension.'}, {'timestamp': (1916.31, 1918.27), 'text': ' Why do we do 1 over theta?'}, {'timestamp': (1918.27, 1921.71), 'text': ' Well, because this is to the power of minus 2.'}, {'timestamp': (1921.71, 1927.55), 'text': ' So something to the power of a negative number is 1 over that something'}, {'timestamp': (1927.55, 1940.34), 'text': ' to the power of the positive exponent and then this will result in a matrix with shape head dimension divided by 2.'}, {'timestamp': (1940.34, 1945.34), 'text': ' So shape is head dimension divided by 2.'}, {'timestamp': (1946.34, 1949.34), 'text': ' Now we construct the positions. What are the positions?'}, {'timestamp': (1949.34, 1954.15), 'text': ' Because we want to build these two matrices, they depend on theta, so the series of theta'}, {'timestamp': (1954.15, 1959.71), 'text': ' that goes from theta 1 to theta dimension divided by 2, and that we already have.'}, {'timestamp': (1959.71, 1961.43), 'text': " Now we need to build the m's."}, {'timestamp': (1961.43, 1966.47), 'text': " Because the m's, the possible positions of a token can be many, we basically"}, {'timestamp': (1966.47, 1973.47), 'text': ' give as input to this function the maximum sequence length that we can afford multiplied'}, {'timestamp': (1973.47, 1977.86), 'text': ' by 2 because we have also the prompt which may be long.'}, {'timestamp': (1977.86, 1984.14), 'text': " So we say ok let's pre-compute all the possible theta and m for all the possible positions"}, {'timestamp': (1984.14, 1989.54), 'text': ' that our model will see and all the possible positions is given by this parameter sequence'}, {'timestamp': (1989.54, 1991.67), 'text': ' length.'}, {'timestamp': (1991.67, 2000.83), 'text': ' So now construct the positions.'}, {'timestamp': (2000.83, 2007.01), 'text': ' And the shape is sequence length which is m.'}, {'timestamp': (2012.01, 2021.54), 'text': ' Now we need to multiply m by all the sequence of thetas but each M with all the theta so for'}, {'timestamp': (2021.54, 2028.1), 'text': ' example if we have M equal to 1 we need M1 theta 1 M1 theta 2 M1 theta divide'}, {'timestamp': (2028.1, 2034.67), 'text': ' D divide by 2 then we need M2 theta 1 m2 theta 2 m2 theta 3 so for that we will use a'}, {'timestamp': (2034.67, 2041.95), 'text': ' outer product the outer product i will show you later basically means multiply all the elements'}, {'timestamp': (2041.95, 2048.01), 'text': ' of the first vector with all the elements of the second vector all the elements of the second vector all the elements of the first vector with all the elements of the second vector, all the possible combinations.'}, {'timestamp': (2048.01, 2049.01), 'text': ' So...'}, {'timestamp': (2072.27, 2075.63), 'text': ' So for example, here we have a frequency is equal to torch, auto product, m and theta. Okay. So what we are doing is we are doing the outer product'}, {'timestamp': (2075.63, 2078.67), 'text': ' within m, which is the positions multiplied by theta.'}, {'timestamp': (2078.67, 2082.27), 'text': ' This will basically take the first element of the first vector'}, {'timestamp': (2082.27, 2085.01), 'text': ' and multiply with all the elements of the second vector.'}, {'timestamp': (2085.01, 2088.97), 'text': ' Then take the second element of the first vector and multiply it with all the'}, {'timestamp': (2088.97, 2094.81), 'text': ' elements of the second vector etc etc. So if we start with the shape, let me say'}, {'timestamp': (2094.81, 2098.94), 'text': ' shape of m is sequence length.'}, {'timestamp': (2101.18, 2114.31), 'text': " Let's say outer product with head dimension divided by 2. This will result in a tensor of sequence length by head"}, {'timestamp': (2114.31, 2118.47), 'text': ' dimension divided by 2 so for each position we will have all the theta then'}, {'timestamp': (2118.47, 2121.63), 'text': ' for the second position we will have all the theta for the third position we will'}, {'timestamp': (2121.63, 2126.25), 'text': ' have all the theta and so on.'}, {'timestamp': (2126.25, 2150.19), 'text': ' Now we want to write these numbers into a complex form and I will show you why. I multiplied by M multiplied by theta'}, {'timestamp': (2150.19, 2153.51), 'text': ' where R is equal to 1'}, {'timestamp': (2153.51, 2180.34), 'text': ' as follows. So we compute Let me also write the shape and then I explain you how it works.'}, {'timestamp': (2180.34, 2187.34), 'text': ' This is here too.'}, {'timestamp': (2187.34, 2193.79), 'text': " Okay, let's write some formulas. I could also, you know, not explain all the"}, {'timestamp': (2193.79, 2197.75), 'text': ' proofs, so I know this in the next few minutes will be a little boring because I'}, {'timestamp': (2197.75, 2202.19), 'text': " will be explaining all the math behind it, but of course I don't think just you"}, {'timestamp': (2202.19, 2206.09), 'text': ' just like me, you like to watch just some code and say'}, {'timestamp': (2206.09, 2211.21), 'text': " okay this is how it's done. No. I like to actually give a motivation behind every"}, {'timestamp': (2211.21, 2214.81), 'text': " operation we do and that's I think one of the reasons you are watching this video and not just"}, {'timestamp': (2214.81, 2217.5), 'text': ' reading the code from the meta repository.'}, {'timestamp': (2217.5, 2223.1), 'text': " So let's do some math."}, {'timestamp': (2223.1, 2227.06), 'text': ' The first thing we need to review is how complex numbers work.'}, {'timestamp': (2227.06, 2239.67), 'text': ' Okay, a complex number is a number in the form a plus i multiplied by b where a is called the real part and the b is called the imaginary part.'}, {'timestamp': (2239.67, 2245.85), 'text': ' And i is a number such that i to the power of 2 is equal to minus 1.'}, {'timestamp': (2245.85, 2253.61), 'text': ' So the complex numbers were introduced to represent all the numbers that involve somehow the square root of a negative number.'}, {'timestamp': (2253.61, 2257.46), 'text': ' As you know from school the square root of a number in number cannot be calculated'}, {'timestamp': (2257.46, 2261.26), 'text': " but so that's why we introduced this constant i which is the negative number"}, {'timestamp': (2261.26, 2265.5), 'text': ' which is the square root of minus 1 and so we can represent the square root of'}, {'timestamp': (2265.5, 2273.67), 'text': ' negative numbers and they can also be helpful in vector calculations and we will see how because'}, {'timestamp': (2273.67, 2290.69), 'text': " because we have the Euler's formula. The Euler's formula says that e to the power of i multiplied by x is equal to cosine of x plus i multiplied by sine of"}, {'timestamp': (2290.69, 2300.06), 'text': ' x. So it allows us to represent a complex number in the exponential form into a sum of two trigonometric functions,'}, {'timestamp': (2300.06, 2307.42), 'text': ' the cosine and the sine, and this will be very helpful later, because our goal, our goal is to'}, {'timestamp': (2307.42, 2314.75), 'text': ' calculate these matrices here the cosine of m theta and the signs of'}, {'timestamp': (2314.75, 2320.91), 'text': ' m theta and the first thing we did is we calculated all the theta one then we'}, {'timestamp': (2320.91, 2324.55), 'text': ' calculated all the positions then we calculated all the positions, then we calculated all the possible'}, {'timestamp': (2324.55, 2331.65), 'text': ' combinations of positions and thetas. So what we did is we calculated a vector'}, {'timestamp': (2331.65, 2343.86), 'text': ' that represents the theta. So theta 1, theta 2 up to theta t divided by 2 then we calculated all the'}, {'timestamp': (2343.86, 2352.63), 'text': " possible m's m can be 1 can be 2 be whatever, so sequence length let's say."}, {'timestamp': (2352.63, 2359.67), 'text': ' Then we calculated the product of each of them for all the possible Theta.'}, {'timestamp': (2359.67, 2398.97), 'text': ' So for example we created a new matrix that has m1 theta 1 m1 theta 2 m1 theta 3 up to m up to m1 theta d divided by 2 and then m2 theta 1 m2 theta 2 m2 theta 3 etc etc until m2 theta d divided by 2. These numbers are still not complex'}, {'timestamp': (2398.97, 2402.85), 'text': ' numbers they are just real numbers because theta is a real number, m is a real number'}, {'timestamp': (2402.85, 2404.17), 'text': ' but they are not complex numbers.'}, {'timestamp': (2404.17, 2406.45), 'text': ' Then we convert them into complex numbers.'}, {'timestamp': (2406.45, 2412.21), 'text': ' So what we do with the last operation here, this one here, we convert each of these numbers'}, {'timestamp': (2412.21, 2419.78), 'text': ' into polar, into its polar form. A number in polar form is a number that can be written as r multiplied'}, {'timestamp': (2419.78, 2428.9), 'text': ' by e to the power of i theta which can be written as r cosine of theta plus i sine of'}, {'timestamp': (2428.9, 2430.11), 'text': ' theta.'}, {'timestamp': (2430.11, 2434.63), 'text': ' Why? Because it can be represented in the graphical,'}, {'timestamp': (2434.63, 2439.13), 'text': " let's say, graphical plane xy."}, {'timestamp': (2440.39, 2442.75), 'text': ' As you know, complex numbers can be represented'}, {'timestamp': (2442.75, 2446.53), 'text': ' into the 2D plane, x xy where the real part is'}, {'timestamp': (2446.53, 2450.97), 'text': ' on the x and the imaginary part is on the y.'}, {'timestamp': (2450.97, 2460.66), 'text': " So we are actually representing a vector of size let's say r with an inclination of theta because as you know the"}, {'timestamp': (2460.66, 2467.78), 'text': ' projection of this vector on the real part is r cos theta plus i the the'}, {'timestamp': (2467.78, 2474.47), 'text': ' projection on the y-axis is sine of theta and here I forgot r yeah I'}, {'timestamp': (2474.47, 2481.11), 'text': ' forgot an r here r sine of theta so this is another way of representing'}, {'timestamp': (2481.11, 2485.25), 'text': ' complex numbers and what we are doing is we are calculating this matrix'}, {'timestamp': (2485.25, 2490.67), 'text': ' and then converting all these numbers into their complex form. So we are converting it'}, {'timestamp': (2490.67, 2505.22), 'text': ' into another matrix that has r equal to 1. And this number here, for for example this item here will become cosine of m1 theta'}, {'timestamp': (2505.22, 2517.23), 'text': ' 1 plus i sine of m1 theta 1. This number here will become another number. So this is'}, {'timestamp': (2517.23, 2530.37), 'text': ' only one number. This is become another complex number that is the cosine of m1 theta2 plus i sine of m1 theta2'}, {'timestamp': (2530.37, 2536.37), 'text': ' etc etc for all the because we are not increasing the numbers the overall'}, {'timestamp': (2536.37, 2539.6), 'text': " the total numbers this shape of the tensor also doesn't change."}, {'timestamp': (2539.6, 2547.34), 'text': ' It just becomes a more complex number. So instead of having m theta 1, it becomes cosine of m theta 1 plus i m theta 1.'}, {'timestamp': (2547.34, 2553.67), 'text': ' Why do we need this form here? Because we need signs and cosines.'}, {'timestamp': (2553.67, 2555.67), 'text': ' And later we will see how we will use them.'}, {'timestamp': (2555.67, 2560.67), 'text': ' Now, the point is, imagine we are giving a vector,'}, {'timestamp': (2560.67, 2563.77), 'text': ' because we want to apply these positional encodings to a vector.'}, {'timestamp': (2564.33, 2571.45), 'text': ' So how to apply them? Because the vector we will be given as a list of dimensions from x1 to the'}, {'timestamp': (2571.45, 2580.98), 'text': " last dimension. Just like in the original transformer we have a vector of size 512. In this case it will be much smaller because it's the dimension of each head."}, {'timestamp': (2580.98, 2586.46), 'text': " And as you remember, each head doesn't watch the full dimension of the embedding vector,"}, {'timestamp': (2586.46, 2588.14), 'text': ' but a part of it.'}, {'timestamp': (2588.14, 2605.53), 'text': " So but for us, okay, imagine it's only one head, so if it's only one head, we will watch the full dimension. part of it. con le dimensioni di freddo. Quindi, in caso di lama, 4.096 dimensioni."}, {'timestamp': (2605.53, 2608.61), 'text': ' E in caso di la trasformazione di vanille 512.'}, {'timestamp': (2608.61, 2609.93), 'text': ' Come applicare?'}, {'timestamp': (2609.93, 2611.77), 'text': " Dov'è fare un metto?"}, {'timestamp': (2611.77, 2614.13), 'text': " In realtà, dov'è fare un metto più."}, {'timestamp': (2614.13, 2616.17), 'text': ' Quindi, abbiamo dato...'}, {'timestamp': (2624.22, 2631.79), 'text': ' Abbiamo dato... format. So, we are given, we are given, suppose a smaller embedding vector because we want to do the calculation and not go crazy, otherwise 4096 is a little difficult to prove.'}, {'timestamp': (2631.79, 2640.79), 'text': ' I want to make a list of operations on this vector until we arrive to this form here.'}, {'timestamp': (2640.79, 2641.79), 'text': " So let's start."}, {'timestamp': (2641.79, 2646.17), 'text': ' Suppose our embedding vector is only made of four dimensions.'}, {'timestamp': (2646.17, 2651.61), 'text': ' X1, X2, X3 and X4.'}, {'timestamp': (2651.61, 2661.14), 'text': ' Okay, the first thing we do is I will do some transformations and I will later translate'}, {'timestamp': (2661.14, 2662.14), 'text': ' them into code.'}, {'timestamp': (2662.14, 2664.98), 'text': " So for now just follow the transformations I'm doing."}, {'timestamp': (2664.98, 2667.96), 'text': ' This is the transformation number one.'}, {'timestamp': (2667.96, 2676.12), 'text': ' I want to group successive dimensions into another dimension.'}, {'timestamp': (2676.12, 2692.65), 'text': ' So x1 and x2 become another dimension in this tensor and x3 and x4 become another dimension in this tensor.'}, {'timestamp': (2692.65, 2704.58), 'text': ' The total number of items is still 4 but I added another dimension and this has size 4 by 1.'}, {'timestamp': (2704.58, 2711.68), 'text': ' This one has 2 by 2 by 1 so I split it into multiple tensors.'}, {'timestamp': (2711.68, 2714.18), 'text': ' And, okay.'}, {'timestamp': (2714.18, 2720.18), 'text': ' Now, the next thing I do, I consider this first number of this part'}, {'timestamp': (2720.18, 2722.68), 'text': ' to be the real part of the complex number,'}, {'timestamp': (2722.68, 2724.89), 'text': ' and this one to be the imaginary part of the complex number and this one to be the imaginary part of'}, {'timestamp': (2724.89, 2729.81), 'text': ' the complex number and the same for the second vector here. So I do another'}, {'timestamp': (2729.81, 2746.06), 'text': ' transformation that we will call 2 in which x1 plus ix2 and then x3 plus ix2.'}, {'timestamp': (2746.06, 2754.36), 'text': ' This vector has less items because now two numbers became one complex number.'}, {'timestamp': (2754.36, 2762.24), 'text': ' Now I multiply this element wise with the vector that we pre-computed before.'}, {'timestamp': (2762.24, 2772.27), 'text': ' As you remember before, we pre-computed this one, cosine of m1 theta1 plus i of m1 theta1, cosine of m1 theta2'}, {'timestamp': (2772.27, 2773.89), 'text': ' plus i.'}, {'timestamp': (2773.89, 2781.22), 'text': ' Because this token here, suppose his position is M1 because we need also the M'}, {'timestamp': (2781.34, 2785.66), 'text': ' So this suppose this token here his position is M1'}, {'timestamp': (2786.26, 2789.68), 'text': ' So we take all this row here M1 and'}, {'timestamp': (2796.04, 2803.16), 'text': ' This will become our new matrix here. We only have four dimensions, so four dimensions means we have theta1 and theta2, because d'}, {'timestamp': (2803.16, 2806.49), 'text': ' divided by 2, until d divided by 2 until d divided by 2 so'}, {'timestamp': (2806.49, 2828.34), 'text': ' element wise with cosine of m1 theta 1 plus i sine of m1 theta1.'}, {'timestamp': (2828.34, 2847.01), 'text': ' And then we have cosine of m1 theta2 plus i of sine of m1 theta2.'}, {'timestamp': (2847.01, 2852.09), 'text': ' Now we have an element-wise product between the first item of this matrix and the first'}, {'timestamp': (2852.09, 2855.53), 'text': ' item of this matrix, actually they are two vectors.'}, {'timestamp': (2855.53, 2858.86), 'text': ' And then we have the product of two complex'}, {'timestamp': (2858.86, 2863.94), 'text': " numbers this complex number here and this complex number here so let's do"}, {'timestamp': (2863.94, 2869.06), 'text': " let's see how to compute the product of two complex numbers because I don't want"}, {'timestamp': (2869.06, 2872.44), 'text': ' to write very long expressions I will call this one'}, {'timestamp': (2873.64, 2882.76), 'text': ' f1. So f1 is the cosine of m1 theta 1 and f2 is the sine of m1 theta 1.'}, {'timestamp': (2884.45, 2887.19), 'text': ' And for the same reason I will call this one f3 and'}, {'timestamp': (2888.41, 2890.41), 'text': ' f4'}, {'timestamp': (2891.57, 2892.89), 'text': ' now'}, {'timestamp': (2892.89, 2896.01), 'text': " Let's compute the product of the first"}, {'timestamp': (2899.5, 2900.22), 'text': ' item of this vector and the first item of this vector.'}, {'timestamp': (2927.01, 2933.81), 'text': ' So x1 plus ix2 multiplied by f1 plus i f2. This is equal to x1 f1 plus i x1,f2 plus i,'}, {'timestamp': (2933.81, 2957.68), 'text': " x2 f1 then we have this product i x2 multiplied by i x2 but it will become i squared x2 f2 i squared we know it's equal to minus one so it will become minus x2 f2."}, {'timestamp': (2957.68, 2965.61), 'text': " This one can then be written as real part so all the terms that don't have i x1 f1"}, {'timestamp': (2967.85, 2971.45), 'text': ' Minus x2 f2'}, {'timestamp': (2972.37, 2974.37), 'text': ' plus i'}, {'timestamp': (2974.41, 2984.34), 'text': ' That multiplies x1f2 x1f2 plus x2f1.'}, {'timestamp': (2984.34, 2991.16), 'text': " Okay, this is how to compute the product of two complex numbers. Let's do it."}, {'timestamp': (2991.16, 2996.28), 'text': ' So the first number here in the resulting matrix from this element-wise multiplication'}, {'timestamp': (2996.28, 3025.9), 'text': ' will be x1f1 x1f1 minus x2f2 plus i of x1f2 plus i of x1f2 plus i of x1f2 plus i of x1f2 plus i of x1f2 plus x2f1.'}, {'timestamp': (3025.9, 3031.72), 'text': " The second element, we don't need to do this multiplication because they have the similar structure as the first one"}, {'timestamp': (3031.72, 3037.64), 'text': ' So we just change the x1 with x3 x2 with x4. This is x4 and'}, {'timestamp': (3040.36, 3074.4), 'text': ' F1 with F3 and F2 with f4. So the resulting matrix will be x3 f3 minus x4 f4 plus i x3 f4 plus x4 f3.'}, {'timestamp': (3074.4, 3079.24), 'text': ' This one can then be split back, so this complex number we can split the real part and the'}, {'timestamp': (3079.24, 3084.01), 'text': ' imaginary part and this we will call it transformation number 3.'}, {'timestamp': (3086.81, 3090.49), 'text': ' So we can split it into two in a tensor of two dimensions.'}, {'timestamp': (3090.49, 3114.08), 'text': ' One is the real part and one is the complex part, where is x1f1 minus x2f2 then x1f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f2 plus x2f1'}, {'timestamp': (3116.08, 3122.96), 'text': ' This is the first tensor the second tensor will be x3f3'}, {'timestamp': (3141.14, 3148.1), 'text': " minus x4 f3 minus x4 f4 and the second will be x3 f4 plus x4 f3. I'm really sorry for the bad handwriting, but it's my touchpad is raw, not so good."}, {'timestamp': (3148.1, 3152.68), 'text': ' Then we do another transformation in which we flatten all these values.'}, {'timestamp': (3152.68, 3157.64), 'text': ' So this will be the first item, this is the second, the third and the fourth.'}, {'timestamp': (3157.64, 3161.0), 'text': ' So we remove this dimension, the inner dimension.'}, {'timestamp': (3161.0, 3172.05), 'text': ' So we flatten this matrix and it will become x1 f1 minus x2 f2.'}, {'timestamp': (3172.05, 3214.25), 'text': ' The second items will be x1 f2 plus x2 x1 f2 plus 2x2f1 then we have x3f3 minus x4f4 then we have x3f4 plus x4f3.'}, {'timestamp': (3214.25, 3228.58), 'text': " Let's compare this resulting matrix with what is in the paper."}, {'timestamp': (3228.58, 3233.36), 'text': ' The resulting matrix is exactly the same as what is in the paper.'}, {'timestamp': (3233.36, 3240.48), 'text': ' So x1 multiplied by f1 which is the cosine as you remember m1 theta1.'}, {'timestamp': (3240.48, 3251.21), 'text': ' So x1 multiplied by f1 plus minus x2, so minus x2 multiplied by f2 which is the sign as you can see here,'}, {'timestamp': (3251.21, 3259.98), 'text': " sign of m1 theta1. Here it's not m1 because it's for the generic m, but we set m equal to m1."}, {'timestamp': (3259.98, 3267.06), 'text': " And the second dimension is also correct, so it's x1f2, so x1 with x1 here, because"}, {'timestamp': (3267.06, 3269.86), 'text': " here is the sum, so the order doesn't matter."}, {'timestamp': (3269.86, 3273.48), 'text': ' So x1f2, so x1 multiplied by the sign plus'}, {'timestamp': (3273.48, 3291.85), 'text': ' x2 f1 x2 f1 and let me check if we can use okay the third dimension is x3 f3, so x3 multiplied by the cosine minus x4 sine minus x4 sine.'}, {'timestamp': (3291.85, 3297.58), 'text': ' Then we have x3f4, so x3f4, f4 is the sine of data 2. And plus x4 is the sign of theta 2 and'}, {'timestamp': (3298.9, 3305.82), 'text': ' Plus x4 f3 x4 f3 f3 is the cosine of theta 2 in also in this case because we have'}, {'timestamp': (3305.9, 3314.6), 'text': " There are we have the sum here inside the the order't matter. So as you can see we started with a vector"}, {'timestamp': (3314.6, 3320.84), 'text': ' of dimension 4 but it could be of dimension n and we did some transformation, we then'}, {'timestamp': (3320.84, 3324.81), 'text': ' multiplied with the matrix that we pre-computed here, then we'}, {'timestamp': (3324.81, 3330.21), 'text': ' did some other transformation and the end result is exactly as doing this operation.'}, {'timestamp': (3330.21, 3335.45), 'text': " So now let's translate this into code, because this is actually what we need to apply the"}, {'timestamp': (3335.45, 3338.78), 'text': ' embedding vector to this vector here.'}, {'timestamp': (3338.78, 3343.1), 'text': ' So to this token, how to apply the embeddings, the rotary position embeddings through this'}, {'timestamp': (3343.1, 3345.26), 'text': ' series of transformations.'}, {'timestamp': (3345.26, 3352.12), 'text': ' So I could have also written the code and not tell you anything, but I like to give proof to what I do so that you know that what'}, {'timestamp': (3352.12, 3358.44), 'text': " I'm doing is actually described in the paper and we are actually doing it according to"}, {'timestamp': (3358.44, 3363.12), 'text': ' the paper. There is also a visualization in the paper that is really helpful. So what'}, {'timestamp': (3363.12, 3373.37), 'text': ' we did here, for example, is we transformed the embedding vector into a new tensor which'}, {'timestamp': (3373.37, 3379.46), 'text': ' has half dimension but by grouping two consecutive dimensions, so the two consecutive'}, {'timestamp': (3379.46, 3383.94), 'text': ' dimensions x1 and x2 and x3 and x4.'}, {'timestamp': (3383.94, 3395.28), 'text': ' Then we multiplied with mθ, and this visualization of why we do this is present'}, {'timestamp': (3395.28, 3402.04), 'text': ' in the paper, in particular at this figure here.'}, {'timestamp': (3402.04, 3411.69), 'text': ' So here they say, if you have a word with n dimensions, you need of course d dimensions,'}, {'timestamp': (3411.69, 3423.24), 'text': ' then of course you will have theta of d half theta, because we have theta1, theta2 up to theta d-halve. We group successive'}, {'timestamp': (3423.24, 3430.0), 'text': ' dimensions into a new complex number that if we project it on the complex plane it will'}, {'timestamp': (3430.0, 3431.56), 'text': ' result into this vector, so'}, {'timestamp': (3431.56, 3435.12), 'text': ' the x1, x2 vector you can see here.'}, {'timestamp': (3435.12, 3439.88), 'text': ' And then we multiply it with the complex number m theta 1.'}, {'timestamp': (3439.88, 3445.77), 'text': ' This will result in the number being rotated by the angle indicated by m theta 1.'}, {'timestamp': (3445.77, 3450.17), 'text': ' And this is the encoded number, is the encoded token.'}, {'timestamp': (3450.17, 3454.81), 'text': ' And this is exactly what we are doing with our matrix transformations that I show you'}, {'timestamp': (3454.81, 3455.81), 'text': ' right now.'}, {'timestamp': (3455.81, 3484.93), 'text': " Now, let's translate this into code. Apply rotary embeddings. to Fricks complex is the output of this function but only for the position of"}, {'timestamp': (3484.93, 3489.37), 'text': ' this token. So only for all the positions of this token because this will'}, {'timestamp': (3489.37, 3492.69), 'text': ' have all the theta for all the possible positions but we only need'}, {'timestamp': (3492.69, 3496.7), 'text': ' the positions for this particular token.'}, {'timestamp': (3496.7, 3502.94), 'text': ' And then we need the device.'}, {'timestamp': (3502.94, 3508.62), 'text': ' The first thing we do is the transformation number 1, I think I call it, yeah, this one'}, {'timestamp': (3508.62, 3509.94), 'text': ' here.'}, {'timestamp': (3509.94, 3510.42), 'text': ' And number 1 and number 2. So the first thing we do is... I think I call it. Yeah, this one here and'}, {'timestamp': (3517.26, 3517.94), 'text': ' number one and number two so the first thing we do is we transform the two consecutive dimensions into a new'}, {'timestamp': (3521.58, 3522.5), 'text': ' tensor and then we visualize it as a complex number these'}, {'timestamp': (3554.8, 3556.3), 'text': ' Operations are supported by PyTorch so we do them. So we create x complex Okay, this operation here is basically saying take two consecutive dimensions and group them.'}, {'timestamp': (3556.3, 3561.08), 'text': ' And then we transform this intermediate tensor into a complex tensor by using the view as'}, {'timestamp': (3561.08, 3565.77), 'text': ' complex operation from Torch.'}, {'timestamp': (3565.77, 3567.05), 'text': ' Let me write some comments.'}, {'timestamp': (3567.05, 3574.69), 'text': ' So we are starting from B, sequence length, edge, head dimension.'}, {'timestamp': (3574.69, 3580.5), 'text': " Because I saw before this X is actually not the original vector but it's already the one divided into"}, {'timestamp': (3581.18, 3584.68), 'text': ' With its head dimension because we we will have a multi head'}, {'timestamp': (3585.98, 3591.84), 'text': ' Multi head attention, but if there is no multi head attention then this head dimension is actually the full'}, {'timestamp': (3591.84, 3598.48), 'text': ' dimension of the token so 4096.'}, {'timestamp': (3598.48, 3604.73), 'text': ' Then we have this tensor here but this tensor has two dimensions less than this one.'}, {'timestamp': (3604.73, 3608.45), 'text': " It doesn't have the batch dimension and it doesn't have the head dimension."}, {'timestamp': (3608.45, 3609.45), 'text': ' So we need to add it.'}, {'timestamp': (3609.45, 3627.02), 'text': " So take the Fricks complex and we add the two dimensions that it's missing and here we are doing okay let me write all the transformations here we are"}, {'timestamp': (3627.02, 3638.52), 'text': ' going from here to divide by 2. Why divide by 2? Because every two consecutive pairs are'}, {'timestamp': (3638.52, 3647.77), 'text': ' becoming one complex number and here we go from sequence length to head dimension divided by 2.'}, {'timestamp': (3647.77, 3654.45), 'text': ' We are mapping it to 1 because this is the batch dimension, sequence length, then the'}, {'timestamp': (3654.45, 3661.02), 'text': ' head dimension 1 and then head dimension divided by 2.'}, {'timestamp': (3661.02, 3666.38), 'text': ' Now we multiply them together, so we do this operation here, so element wise multiplication,'}, {'timestamp': (3666.38, 3671.72), 'text': " which will result in a rotation as we saw in the figure before so that's why I call it x-rotated is"}, {'timestamp': (3671.72, 3679.4), 'text': ' equal to x complex multiplied by the complex number of the frequencies. In this case we'}, {'timestamp': (3679.4, 3711.68), 'text': ' are doing sequence length h head dimension divided by 2 Then we multiply it, we obtain this result and then we first transform it into...'}, {'timestamp': (3711.68, 3724.77), 'text': ' We transform the complex number into a tensor in which the first item is the real part of the complex number and then the complex part, the imaginary part, and then we flatten it.'}, {'timestamp': (3724.77, 3734.97), 'text': " So let's do it."}, {'timestamp': (3734.97, 3766.13), 'text': ' This operation view as real will transform the tensor number into a tensor of two dimensions.'}, {'timestamp': (3766.13, 3769.25), 'text': " That's why you can see this additional dimension here."}, {'timestamp': (3769.25, 3810.51), 'text': ' And then we flatten it. You can just say to flatten it with the shape of the original tensor. Become. And this is how we calculate the embedding.'}, {'timestamp': (3810.51, 3832.12), 'text': ' So given a tensor of representing a token or a list of tokens because we have the batch dimensions, we need to all equivalent to doing this operation as written on the paper.'}, {'timestamp': (3832.12, 3838.92), 'text': ' Now we need to go forward with our transformer by implementing the rest.'}, {'timestamp': (3838.92, 3847.61), 'text': " The next thing that we can implement is this RMS norm because it's present at the output of the transformer but also"}, {'timestamp': (3847.61, 3848.61), 'text': ' at the input.'}, {'timestamp': (3848.61, 3851.65), 'text': " So let's go review again the architecture."}, {'timestamp': (3851.65, 3860.67), 'text': " We can see that we have the RMS normalization here but we also have it here and here, so let's implement"}, {'timestamp': (3860.67, 3861.67), 'text': ' it.'}, {'timestamp': (3861.67, 3864.95), 'text': " Let's also visualize how the RMS norm works."}, {'timestamp': (3864.95, 3870.76), 'text': ' If you want to have a deep understanding of how normalization works, in my previous video about Lama, I actually'}, {'timestamp': (3870.76, 3876.34), 'text': ' described why we need normalization, how it was historically done and how it works.'}, {'timestamp': (3876.34, 3882.12), 'text': ' Also at the autograd level. So I will not repeat the same lecture here. I will just'}, {'timestamp': (3882.12, 3885.37), 'text': ' briefly introduce how it works. But if you want to have a better'}, {'timestamp': (3885.37, 3890.01), 'text': ' understanding please watch my previous video. So as you remember in the original transformer'}, {'timestamp': (3890.01, 3896.49), 'text': ' we used layer normalization. Layer normalization works like this. We have an input where we have'}, {'timestamp': (3896.49, 3899.67), 'text': ' some items, suppose item 1, item 2 up to item'}, {'timestamp': (3899.67, 3906.63), 'text': ' 10. Each item has 3 features, so A1, A2, A3. What we did with layer normalization, we'}, {'timestamp': (3906.63, 3914.04), 'text': ' computed two statistics, one for each item, so mu and sigma, so the mean and the sigma,'}, {'timestamp': (3914.04, 3921.48), 'text': ' and we standardize each item, normalize each element of this input matrix using this formula'}, {'timestamp': (3921.48, 3926.97), 'text': ' here, which transforms it into a distribution with zero mean and variance'}, {'timestamp': (3926.97, 3927.97), 'text': ' of one.'}, {'timestamp': (3927.97, 3931.81), 'text': ' And this formula comes from probability statistics.'}, {'timestamp': (3931.81, 3940.23), 'text': ' So as you know, if you have any random variable with its mu and sigma,, se fai la variabile minus la mia divina'}, {'timestamp': (3940.23, 3943.97), 'text': ' da la deviazione standard, quindi la variazza di la strada,'}, {'timestamp': (3943.97, 3950.68), 'text': ' sarà un risultato in un gaudio di min 0 e la var varia di 1.'}, {'timestamp': (3950.68, 3956.68), 'text': ' Poi multipliamo questo con il parameter gamma e aggiungiamo anche un parameter beta qui.'}, {'timestamp': (3956.68, 3960.68), 'text': ' Ma questo è fatto in normalizzazione di strada.'}, {'timestamp': (3960.68, 3967.01), 'text': " In Lama usiamo la normalizzization and let's see the difference."}, {'timestamp': (3967.01, 3976.01), 'text': " In RMS normalization the paper of the RMS normalization claims that we don't need to obtain the effect of layer normalization,"}, {'timestamp': (3976.01, 3978.47), 'text': " we don't need to compute two statistics,"}, {'timestamp': (3978.47, 3985.75), 'text': ' that is the mean and the variance, and actually they claim that the effect given by layer normalization'}, {'timestamp': (3985.75, 3992.6), 'text': ' can be obtained without recentering the values, so without recentering them around the mean of 0.'}, {'timestamp': (3992.6, 3996.22), 'text': ' But just by scaling, however, the'}, {'timestamp': (3996.22, 3998.18), 'text': ' variance in the layer normalization was'}, {'timestamp': (3998.18, 4000.1), 'text': ' computed using the mean, because if you'}, {'timestamp': (4000.1, 4001.92), 'text': ' remember the formula of the variance is'}, {'timestamp': (4001.92, 4006.21), 'text': ' x minus the mean of the distribution to the power of 0. dacă se înțelegi formula de varianță este x-ul de distribuție'}, {'timestamp': (4006.21, 4008.53), 'text': ' pe power de 2 divided by n.'}, {'timestamp': (4008.53, 4017.35), 'text': " Deci, să computeți varianță, avem de la dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de dea de de dea de dea de dea de dea de dea de dea de de dea de dea de de dea de de dea de de dea de de dea de de dea de de dea de de de dea de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de mean. But we wanted to avoid computing the mean. Because we don't need it."}, {'timestamp': (4017.35, 4019.55), 'text': ' This is not what the RMS paper claims.'}, {'timestamp': (4019.55, 4022.71), 'text': " The RMS paper claims that we don't need the mean."}, {'timestamp': (4022.71, 4023.95), 'text': " And we don't need to recenter."}, {'timestamp': (4023.95, 4027.79), 'text': " So we need to compute a statistic that doesn't depend on the mean."}, {'timestamp': (4027.79, 4031.6), 'text': " That's why they introduced this statistics here, which is the root mean square that doesn't depend on the mean. That's why they introduced these statistics here, which is the root mean square"}, {'timestamp': (4031.6, 4037.4), 'text': " that doesn't depend on the mean, and in practice gives the same normalization effect as the"}, {'timestamp': (4037.4, 4044.61), 'text': " layer normalization. And we also have a gamma parameter also here that is learnable and that's multiplied."}, {'timestamp': (4044.61, 4048.77), 'text': ' So as you can see the only difference between the layer normalization and RMS normalization'}, {'timestamp': (4048.77, 4051.41), 'text': " is that we don't re-center the values."}, {'timestamp': (4051.41, 4057.79), 'text': ' And looks like that re-centering was not necessary, as written in the paper, because they say in this paper we hypothesized'}, {'timestamp': (4057.79, 4063.35), 'text': ' that the rescaling invariance is the reason for the success of layer norm rather than'}, {'timestamp': (4063.35, 4065.59), 'text': ' the recentering invariance.'}, {'timestamp': (4065.59, 4072.68), 'text': ' So they just rescaled the values according to the RMS statistic and this is what we will do in our'}, {'timestamp': (4072.68, 4073.68), 'text': ' code.'}, {'timestamp': (4073.68, 4107.91), 'text': " So let's build this block. Thank you. So the APS value you can see here is used as a denominator."}, {'timestamp': (4107.91, 4109.51), 'text': ' Let me go back here.'}, {'timestamp': (4109.51, 4119.64), 'text': " It's used here as the added to the denominator to avoid a division by 0."}, {'timestamp': (4119.64, 4133.09), 'text': ' And then we have the gamma parameter.'}, {'timestamp': (4133.09, 4134.09), 'text': " And that's it."}, {'timestamp': (4134.09, 4157.68), 'text': ' Then we define the function norm where x is a batch sequence length dimension okay so return x by torch dot r s'}, {'timestamp': (4157.68, 4190.68), 'text': " qr t r s qr t stands for the one over the square root. And that's it. To jest to, że jest w porządku. To jest to, że jest w porządku."}, {'timestamp': (4190.68, 4192.68), 'text': ' To jest to, że jest w porządku.'}, {'timestamp': (4192.68, 4194.68), 'text': ' To jest to, że jest w porządku.'}, {'timestamp': (4194.68, 4196.68), 'text': ' To jest to, że jest w porządku.'}, {'timestamp': (4196.68, 4198.68), 'text': ' To jest to, że jest w porządku.'}, {'timestamp': (4198.68, 4200.68), 'text': ' To jest to, że jest w porządku.'}, {'timestamp': (4200.68, 4202.68), 'text': ' To jest to, że jest w porządku.'}, {'timestamp': (4202.68, 4207.01), 'text': ' To jest to, że jest w porządku. To jest to, że jest w porządku. To jest to, że jest w porządku. To jest to, że jest w porządku. To jest to, że jest w porządku. To jest to, że jest w porządku. we multiply by gamma'}, {'timestamp': (4216.65, 4222.33), 'text': ' so we have a as you can see, weight is actually is a number, a list of ones with the dimension'}, {'timestamp': (4222.33, 4234.2), 'text': ' dim, so dim multiplied by the sequence length dim results in B sequence length in where b is the batch dimension'}, {'timestamp': (4235.24, 4245.61), 'text': ' and here what we are doing is this is our sqrt is equal to 1 over sqrt of x just as a reminder.'}, {'timestamp': (4245.61, 4254.97), 'text': ' And the dimensions here are multiplied by bs-length 1,'}, {'timestamp': (4254.97, 4263.59), 'text': " which results in bs-length dimensions. So what we're going to do is we're going to write the dimensions of the dimensions of x. So we're going to exactly just this formula here."}, {'timestamp': (4263.59, 4276.12), 'text': ' So just multiply it by 1 over rms and then multiply it with gamma here. Now that we have also built the RMS norm,'}, {'timestamp': (4276.12, 4283.49), 'text': " let's go check our next building block, which is this encoder block. So what is the encoder block? Let's go back to the transformer."}, {'timestamp': (4284.09, 4290.97), 'text': ' Here we have the encoder block is all this block here that contains a normalization.'}, {'timestamp': (4291.41, 4296.13), 'text': ' It contains a self-attention here. It contains skip connections.'}, {'timestamp': (4296.13, 4300.07), 'text': ' You can see here another normalization another skip connection'}, {'timestamp': (4300.07, 4306.47), 'text': ' and a feed forward layer here i think the easiest one to start with is the'}, {'timestamp': (4307.91, 4313.56), 'text': " fall feed forward but we can also we can also okay let's start first build"}, {'timestamp': (4313.56, 4318.84), 'text': ' the encoder block and then we will build the attention and finally the feed'}, {'timestamp': (4318.84, 4324.21), 'text': ' forward so we first build the skeleton of this then the attention and then this.'}, {'timestamp': (4324.21, 4372.49), 'text': " Let's go! සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� I received some parameters. meters."}, {'timestamp': (4372.49, 4376.95), 'text': ' What is the head dimension of the vector divided by the number of heads.'}, {'timestamp': (4376.95, 4382.15), 'text': ' So 4096 divided by 32.'}, {'timestamp': (4382.15, 4386.23), 'text': ' Because as we can see here, we have the dimension'}, {'timestamp': (4386.23, 4388.67), 'text': ' of the vector of the embedding vector is 4096,'}, {'timestamp': (4388.67, 4389.79), 'text': ' but we have 32 heads.'}, {'timestamp': (4389.79, 4392.12), 'text': ' So each head will see 4096 divided'}, {'timestamp': (4392.12, 4407.33), 'text': ' by 32 items from each token. Then we have a self-attention block.'}, {'timestamp': (4407.33, 4411.29), 'text': " I define it but don't build it right now, just define the skeleton."}, {'timestamp': (4411.29, 4479.68), 'text': " Then we have the normalization before the self-attention. This is our RMS norm and this is the motivation behind this argument norm APS. after it's after the attention not after the feed forward so before the feed forward block And then we have norm.ps."}, {'timestamp': (4479.68, 4480.68), 'text': ' Okay.'}, {'timestamp': (4480.68, 4481.68), 'text': ' Okay.'}, {'timestamp': (4481.68, 4492.77), 'text': " Now let's implement the forward method."}, {'timestamp': (4492.77, 4495.17), 'text': ' Start pausing indicates the position of the token.'}, {'timestamp': (4495.17, 4529.77), 'text': " I kept the same variable number as in the original code. It's actually the position of the token. it's we are dealing with. So, we need to skip connection and here."}, {'timestamp': (4529.77, 4537.99), 'text': ' Okay, the hidden is equal to x plus the attention, so we calculate the attention.'}, {'timestamp': (4537.99, 4540.11), 'text': ' So we calculate the attention of what?'}, {'timestamp': (4540.11, 4543.35), 'text': ' Of the normalized version of this input.'}, {'timestamp': (4543.35, 4555.36), 'text': ' So we first apply the normalization. And then we calculate this attention. to the attention we also give the frequencies'}, {'timestamp': (4555.36, 4563.24), 'text': ' because as you remember the rotary positional encodings are kind of they come into play'}, {'timestamp': (4563.24, 4566.01), 'text': ' when we calculate the attention.'}, {'timestamp': (4566.01, 4573.01), 'text': ' These operations involve tensors of size b sequence length dimension which is x plus the skip connection'}, {'timestamp': (4573.01, 4582.13), 'text': ' plus the output of the attention, b sequence length dimension, which results in b sequence'}, {'timestamp': (4582.13, 4585.23), 'text': ' length dimension.'}, {'timestamp': (4585.23, 4591.16), 'text': ' Then we have the application of the feed forward with its skip connection, so out is equal'}, {'timestamp': (4591.16, 4600.84), 'text': ' to H plus.'}, {'timestamp': (4600.84, 4609.61), 'text': ' And before we send it to the feedforward, as before, we applied the normalization.'}, {'timestamp': (4609.61, 4611.17), 'text': ' And this is the output.'}, {'timestamp': (4611.17, 4614.61), 'text': ' Now we need to build the self-attention and the feedforward.'}, {'timestamp': (4614.61, 4618.79), 'text': " Let's start with the harder part first, so the self-attention, because I think it's"}, {'timestamp': (4618.79, 4619.99), 'text': ' more interesting.'}, {'timestamp': (4619.99, 4625.35), 'text': " Before we build the self-attention, let's review how self-attention worked in the original"}, {'timestamp': (4625.35, 4629.27), 'text': ' transformer and how it will work here.'}, {'timestamp': (4629.27, 4634.68), 'text': ' So, okay, this is the original paper from the original paper of the transformer,'}, {'timestamp': (4634.68, 4638.6), 'text': " so attention is all you need. Let's review the self-attention mechanism in the original"}, {'timestamp': (4638.6, 4657.03), 'text': ' transformer and then we will see how it works in Lama. to query key and values which are the same input.'}, {'timestamp': (4657.03, 4661.03), 'text': ' We multiply by a w matrix which is a parameter matrix'}, {'timestamp': (4661.03, 4663.63), 'text': ' which results in a new matrix which has this dimension'}, {'timestamp': (4663.63, 4665.83), 'text': ' of sequence by d model.'}, {'timestamp': (4665.83, 4669.47), 'text': ' And we then split them into the number of heads'}, {'timestamp': (4669.47, 4673.68), 'text': ' that we have such that each vector that represents'}, {'timestamp': (4673.68, 4679.96), 'text': ' the token is split into suppose we have four heads so each vector each head will see a'}, {'timestamp': (4679.96, 4686.57), 'text': ' part of the embedding of each token. So if the token was 512 in size for example, the embedding'}, {'timestamp': (4686.57, 4695.13), 'text': ' vector, the first head will watch 128 dimensions of this vector, the second head will watch the next'}, {'timestamp': (4695.13, 4701.35), 'text': ' 108 dimensions, the next head will watch the next 108 dimensions, the next head will watch the next 108 dimensions, etc.'}, {'timestamp': (4701.35, 4707.35), 'text': ' We then calculated the attention between all these smaller matrices, so Q, K and V.'}, {'timestamp': (4707.35, 4711.16), 'text': ' These results in head 1, head 2, head 3 and head 4.'}, {'timestamp': (4711.16, 4720.44), 'text': ' We then concatenate them together, we multiply with the w matrix and this is the output of'}, {'timestamp': (4720.44, 4722.6), 'text': ' the multi-head attention.'}, {'timestamp': (4722.6, 4724.75), 'text': " In this case it's called self-attention because"}, {'timestamp': (4724.75, 4731.51), 'text': " it's the same input that acts as a query as key and values. In case the query"}, {'timestamp': (4731.51, 4735.79), 'text': ' comes from one place and the key and the values come from another place in that'}, {'timestamp': (4735.79, 4737.67), 'text': " case it's called cross-attention."}, {'timestamp': (4737.67, 4744.03), 'text': ' And that kind of attention is used in multimodal architectures, for example, when you want'}, {'timestamp': (4744.03, 4752.68), 'text': ' to combine pictures with captions or music with text, or you want to translate from one language to another so you have kind'}, {'timestamp': (4752.68, 4756.52), 'text': ' of multimodality and you want to connect the two together.'}, {'timestamp': (4756.52, 4762.4), 'text': ' But in our case we are modeling a language so self-attention is what we need.'}, {'timestamp': (4762.4, 4766.13), 'text': " Actually attention is all we need so let's watch the"}, {'timestamp': (4767.21, 4769.21), 'text': ' How it works in llama'}, {'timestamp': (4769.61, 4774.69), 'text': ' Okay in llama we need to talk about a lot of things before we build the self-attention'}, {'timestamp': (4774.69, 4777.71), 'text': ' We need to review how the self-attention works in Lama,'}, {'timestamp': (4777.71, 4781.95), 'text': ' how is the KVcache, what is the grouped query attention,'}, {'timestamp': (4781.95, 4784.87), 'text': ' and actually how the inference works.'}, {'timestamp': (4784.87, 4786.07), 'text': ' So we need to review all this stuff'}, {'timestamp': (4786.07, 4787.27), 'text': ' before we proceed with the code.'}, {'timestamp': (4787.27, 4790.24), 'text': ' Otherwise, it will be very hard to follow the code.'}, {'timestamp': (4790.24, 4795.84), 'text': " So let's first talk about the inferencing."}, {'timestamp': (4795.84, 4802.92), 'text': ' Given suppose we have a model that has been trained on this particular line, so the line'}, {'timestamp': (4802.92, 4820.79), 'text': ' is love that can quickly seize the gentle heart. of 5th canto.'}, {'timestamp': (4820.79, 4823.51), 'text': ' We have a model that has been trained on this line.'}, {'timestamp': (4823.51, 4826.99), 'text': ' Love that can quickly seize the gentle heart.'}, {'timestamp': (4826.99, 4832.12), 'text': ' Now a model that has been trained on this particular line using the next token prediction'}, {'timestamp': (4832.12, 4838.12), 'text': ' should have an input that is built in this way, so the start of sentence, and then the'}, {'timestamp': (4838.12, 4842.84), 'text': ' tokens that represent the sentence, then the target should be the same sentence with the'}, {'timestamp': (4842.84, 4844.74), 'text': ' end of sentence. Because the transformer is a sequence to sequence, it should be the same sentence with the end of sentence.'}, {'timestamp': (4844.74, 4849.7), 'text': ' Because the transformer is a sequence-to-sequence model, it maps one input sequence into an'}, {'timestamp': (4849.7, 4852.86), 'text': ' output sequence of the same size.'}, {'timestamp': (4852.86, 4874.6), 'text': " This means that the first token will be mapped to the first token of the output, the second token of the input will be mapped to the second that token of the output, but it's not a one-to-one correspondence because of the mask of the causal"}, {'timestamp': (4874.6, 4882.28), 'text': ' mask that we apply during the self-attention. To predict this particular token, Ken, for example,'}, {'timestamp': (4882.28, 4886.74), 'text': " the model doesn't only watch the same token in the input,"}, {'timestamp': (4886.74, 4890.74), 'text': ' so that, but also watch all the previous tokens.'}, {'timestamp': (4890.74, 4900.19), 'text': ' So the model to predict can needs to access not only that, but also SOS loves that. And the self-attention mechanism with its'}, {'timestamp': (4900.19, 4906.71), 'text': ' causal mask will access all the previous tokens, but not the next ones. This means that when'}, {'timestamp': (4906.71, 4912.96), 'text': ' we do the inferencing we should do it like this. We start with the start of sentence and the model will output the first word.'}, {'timestamp': (4912.96, 4919.08), 'text': ' To output the next token we need to give the previous output token as input also.'}, {'timestamp': (4919.08, 4924.74), 'text': ' So we always append the last token of the output to the input to predict the successive tokens.'}, {'timestamp': (4924.74, 4929.94), 'text': ' So for example to output that we need to give SOS love. To output the next token'}, {'timestamp': (4929.94, 4935.56), 'text': ' we take this that and we put it in the input so that we can get the next word.'}, {'timestamp': (4935.56, 4941.35), 'text': ' To output the next token we need to append the previous output to the input'}, {'timestamp': (4941.35, 4944.35), 'text': ' to get the new output quickly.'}, {'timestamp': (4944.35, 4948.35), 'text': ' Now, when we do this job, the model is actually'}, {'timestamp': (4948.35, 4953.04), 'text': ' we are giving this input, which is a sequence of four tokens, and the model is actually giving this input, which is a sequence of four tokens, and the model is actually giving this input, which is a sequence of four tokens, and the model is actually giving this input input which is a sequence of four tokens and the'}, {'timestamp': (4953.04, 4958.0), 'text': ' model will produce a sequence of four tokens as output.'}, {'timestamp': (4958.0, 4962.64), 'text': ' But this is not really convenient when we do the inferencing because the model is doing'}, {'timestamp': (4962.64, 4966.5), 'text': ' a lot of dot products that are not necessary,'}, {'timestamp': (4966.5, 4969.02), 'text': ' that have already been built.'}, {'timestamp': (4969.02, 4975.54), 'text': ' For example, what I want to say is that in order to get this last token quickly, we'}, {'timestamp': (4975.54, 4979.11), 'text': " need to access all the previous context here. But we don't need to access all the previous context here."}, {'timestamp': (4979.11, 4982.01), 'text': " But we don't need to output love that can."}, {'timestamp': (4982.01, 4986.07), 'text': " Because we don't care, we already have these tokens, we only care about the last one."}, {'timestamp': (4986.07, 4990.68), 'text': " However, we can't just tell the transformer model to not output the previous tokens."}, {'timestamp': (4990.68, 4996.68), 'text': ' We need to change the calculations in such a way that we only receive at the output of the transformer'}, {'timestamp': (4996.68, 5001.88), 'text': ' only one token, so that all the other tokens are not even calculated and this will make the'}, {'timestamp': (5001.88, 5007.02), 'text': ' interesting fast. And this is the job of the KV cache. Let me show you with some diagrams'}, {'timestamp': (5011.38, 5013.78), 'text': ' As you can see at every step of the token'}, {'timestamp': (5013.78, 5030.76), 'text': ' We are only interested in the last token output by the model because we already have the previous ones however the model the access all the prompts to output the next token.'}, {'timestamp': (5030.76, 5035.16), 'text': ' And we do this using the KVcache to reduce the amount of computation.'}, {'timestamp': (5035.16, 5037.6), 'text': " So let's do with some examples."}, {'timestamp': (5037.6, 5047.7), 'text': ' Suppose we do the same job that we did before, so the inferencing of that model. We give the first token, so the SOS, this is the self-attention,'}, {'timestamp': (5047.7, 5052.42), 'text': ' so it will be multiplied by the transpose of the keys, this will produce this matrix here,'}, {'timestamp': (5052.42, 5060.47), 'text': ' which is 1 by 1, so you can check the dimensions, 1 by 4096 multiplied by 4006 by 1 will output a matrix'}, {'timestamp': (5060.47, 5063.15), 'text': ' that is 1 by 1.'}, {'timestamp': (5063.15, 5067.39), 'text': ' This will be multiplied by the values and this will result in the output token.'}, {'timestamp': (5067.39, 5071.4), 'text': ' So this is the inferencing step 1 in which the only token we give is the start'}, {'timestamp': (5071.4, 5079.28), 'text': ' of sentence. Then we take this token output and the token at the output. This is actually'}, {'timestamp': (5079.28, 5083.24), 'text': ' not the token because this has to be mapped to the linear layer etc. But suppose this'}, {'timestamp': (5083.24, 5089.18), 'text': ' is already the token and we append it to the input.'}, {'timestamp': (5089.18, 5093.14), 'text': ' So it becomes the second input of the input.'}, {'timestamp': (5093.14, 5100.07), 'text': ' So this is SOS and this is the last output. We multiply it by the transpose of the keys, we get this'}, {'timestamp': (5100.07, 5106.39), 'text': ' matrix here, we multiply it by the values and we get two output tokens as output, because'}, {'timestamp': (5106.39, 5114.6), 'text': " it's a sequence-to-sequence model. Then we append the output of the previous as the input and"}, {'timestamp': (5114.6, 5119.04), 'text': ' we multiply it by the transpose by the keys, we get this matrix here, we then multiply'}, {'timestamp': (5119.04, 5126.02), 'text': " it by the v's and we get 3 tokens as output. We then append the output of the last one at the queue, we multiply"}, {'timestamp': (5126.02, 5130.34), 'text': ' it by the transpose of the keys, we get this matrix here, we multiply it by the v and we'}, {'timestamp': (5130.34, 5135.34), 'text': ' get this sequence as output. But we see some problems. And the one that I told you before,'}, {'timestamp': (5135.34, 5137.91), 'text': " we are doing a lot of computations that we don't need."}, {'timestamp': (5137.91, 5145.67), 'text': ' First of all, these dot products that we are computing here because this is the self-attention,'}, {'timestamp': (5145.67, 5151.24), 'text': ' so Q multiplied by the transpose of the keys will result in a lot of dot products that result in this'}, {'timestamp': (5151.24, 5157.08), 'text': ' matrix. These dot products that you see here highlighted in Violet have been already computed'}, {'timestamp': (5157.08, 5161.0), 'text': ' at the previous steps, because we are at the step number 4, but these have already been'}, {'timestamp': (5161.0, 5165.22), 'text': ' computed at the previous steps. Plus plus not only they have been computed already'}, {'timestamp': (5165.22, 5171.78), 'text': " we don't need them because we only are interested in what the latest token that we added as the input"}, {'timestamp': (5171.78, 5179.55), 'text': " so to the prompt what it's these tokens dot product with all the other tokens?"}, {'timestamp': (5179.55, 5185.51), 'text': ' Because this tokens.product with all the other tokens will result in the output of the last'}, {'timestamp': (5185.51, 5188.71), 'text': ' token, the one we are interested in.'}, {'timestamp': (5188.71, 5194.04), 'text': ' So if there is a way to not do all these computations again and also'}, {'timestamp': (5194.04, 5198.72), 'text': " to not output all the previous tokens that we actually don't need because we always access"}, {'timestamp': (5198.72, 5212.5), 'text': ' the last latest token, yes, we just use the KVcache. In the KVcache, what we do is we always take the last token and we use it as input so we'}, {'timestamp': (5212.5, 5217.55), 'text': " don't append it to the query, we just use it directly as query. But because the query needs to access per la quera, usiamo solo direttamente come quera."}, {'timestamp': (5217.55, 5221.85), 'text': ' Ma perché la quera ha bisogno di accesso tutti i tocchi precedenti,'}, {'timestamp': (5221.85, 5223.75), 'text': ' abbiamo preso i tuoi chiari e i valori.'}, {'timestamp': (5223.75, 5227.35), 'text': " Quindi abbiamo appendito l'ultimo input per i tuoi chiari e i valori,"}, {'timestamp': (5227.35, 5229.55), 'text': ' ma non abbiamo appendito le quere.'}, {'timestamp': (5229.55, 5233.68), 'text': " Rappendiamo l'ultimo con le quere. the values but we don't append it to the queries, we replace it entirely with the queries."}, {'timestamp': (5233.68, 5236.12), 'text': " Let's see with an example."}, {'timestamp': (5236.12, 5241.6), 'text': ' For example, this is our first step of inferencing, so this is just the start of sentence token.'}, {'timestamp': (5241.6, 5245.78), 'text': ' So we just have one token, we multiply it by the transpose of the keys,'}, {'timestamp': (5245.78, 5250.78), 'text': ' it will result in one by one, so we only have one token as outward. This token, in the previous'}, {'timestamp': (5250.78, 5256.83), 'text': ' case, was appended to the queries, so it became in the next step it became a'}, {'timestamp': (5261.81, 5262.03), 'text': ' Matrix of dimension 2 by 4,000 96, but in our case at the time step 2'}, {'timestamp': (5266.91, 5267.67), 'text': " We don't append it. We only append it to the end of the keys and the values and"}, {'timestamp': (5269.67, 5270.6), 'text': ' We only keep the queries here.'}, {'timestamp': (5270.6, 5275.08), 'text': ' If we do this product again now, we will see that this row here is the only one we are'}, {'timestamp': (5275.08, 5276.08), 'text': ' interested in.'}, {'timestamp': (5276.08, 5279.12), 'text': ' So the one that was not violet in the previous diagram.'}, {'timestamp': (5279.12, 5299.45), 'text': ' And if we do this dot product, it will result in only the last token, the one we are interested in. And every time we keep doing this job. this token, but the number of dot products that we are doing during the infressing is much'}, {'timestamp': (5299.45, 5300.45), 'text': ' less.'}, {'timestamp': (5300.45, 5302.85), 'text': " We don't need to do all those dot products that we did before."}, {'timestamp': (5302.85, 5308.65), 'text': ' So compare this is time step 4, this is 4 dot products, compare it with the previous time'}, {'timestamp': (5308.65, 5309.65), 'text': ' step 4.'}, {'timestamp': (5309.65, 5313.2), 'text': ' So here. Here we have 16 tought products.'}, {'timestamp': (5313.2, 5320.48), 'text': " So we reduce it by a factor of 4 and so that's why it's much faster to do"}, {'timestamp': (5320.48, 5338.31), 'text': " inferencing with the KV cache. we are not computing it again so that's why this is much faster we only compute the one we need"}, {'timestamp': (5338.87, 5344.23), 'text': ' and we only get one token as output if this mechanism is not clear please watch my previous'}, {'timestamp': (5344.23, 5356.44), 'text': " video about Lama in which I describe it in much more detail and also with much more visualizations. Now let's go build this. There is another thing"}, {'timestamp': (5356.44, 5361.16), 'text': ' actually I want to show you before we go to build it which is the grouped query attention.'}, {'timestamp': (5361.88, 5378.65), 'text': " This one here. So I call it grouped multi-query attention because the grouped query attention in the paper also it's called grouped query attention"}, {'timestamp': (5380.11, 5385.73), 'text': ' Now the reason we introduced the grouped query attention is first of all'}, {'timestamp': (5385.73, 5392.24), 'text': ' We had the multi query attention the multi query attention basically were introduced to solve one problem.'}, {'timestamp': (5392.24, 5396.88), 'text': ' That is, we first had the multi head attention.'}, {'timestamp': (5396.88, 5400.96), 'text': ' We introduced the KV cache with the multi head attention.'}, {'timestamp': (5400.96, 5402.56), 'text': ' Just the one we just saw.'}, {'timestamp': (5402.56, 5406.74), 'text': ' The problem was that with the multi head attention we were doing too many dot products.'}, {'timestamp': (5406.74, 5410.72), 'text': ' With the multi-head with KVcache we do less dot products.'}, {'timestamp': (5410.72, 5413.54), 'text': ' This resulted in a lot less computation.'}, {'timestamp': (5413.54, 5416.79), 'text': ' But it also resulted in a new bottleneck for the algorithm.'}, {'timestamp': (5416.79, 5419.87), 'text': ' So the bottleneck was not longer the number of computations,'}, {'timestamp': (5419.87, 5425.51), 'text': ' but how many memory access we were performing to access these tensors.'}, {'timestamp': (5425.51, 5435.68), 'text': ' Because in the GPU, the GPU is much faster at doing computations than it is at moving tensors around in its memory.'}, {'timestamp': (5435.68, 5442.68), 'text': ' So when we are optimizing an algorithm, we not only need to consider how many operations we are doing,'}, {'timestamp': (5442.68, 5445.14), 'text': ' but also how many tensors we are accessing'}, {'timestamp': (5445.14, 5447.18), 'text': ' and where are these tensors located.'}, {'timestamp': (5447.18, 5452.3), 'text': " So it's not a good idea to keep copying tensor from one place to another because the GPU is"}, {'timestamp': (5452.3, 5458.11), 'text': ' much slower at copying memory from one place to another than it is at computing operations.'}, {'timestamp': (5458.11, 5462.91), 'text': ' And this can be visualized on the datasheet of the GPU.'}, {'timestamp': (5462.91, 5469.31), 'text': ' You can see for example that computing operations is 19.5 tf operations per second and while'}, {'timestamp': (5469.31, 5472.44), 'text': ' the memory bandwidth, so how fast it can move memory'}, {'timestamp': (5472.44, 5485.38), 'text': " is 40 times slower. So it's we all we need to optimize algorithms also for managing how many tensors we access and how we move them around the memory. This is why we"}, {'timestamp': (5485.38, 5489.1), 'text': ' added the we introduced the multi-query attention. The multi-query attention'}, {'timestamp': (5489.1, 5495.4), 'text': ' basically means that we have many heads for the queries but we only have one'}, {'timestamp': (5495.4, 5498.35), 'text': ' head for the key and the values.'}, {'timestamp': (5498.35, 5505.67), 'text': ' This resulted in a new algorithm that was much more efficient than the algorithm just'}, {'timestamp': (5505.67, 5507.51), 'text': ' with the KV cache.'}, {'timestamp': (5507.51, 5511.38), 'text': ' Because the KV cache, yeah, it reduced the number of dot products but it had a new'}, {'timestamp': (5511.38, 5515.8), 'text': ' bottleneck, that is the number of memory access. With this algorithm we also may optimize the'}, {'timestamp': (5515.8, 5521.74), 'text': ' memory access but we lose some quality because we are reducing the number of heads for the'}, {'timestamp': (5521.74, 5524.14), 'text': ' key and the values. So we are reducing the number of heads for the key and the values. So we are reducing the'}, {'timestamp': (5524.14, 5531.74), 'text': ' number of parameters in the model. And this way, the model, because we are reducing the'}, {'timestamp': (5531.74, 5538.87), 'text': ' number of parameters involved in the attention mechanism, of course, the model will degrade in quality.'}, {'timestamp': (5538.87, 5543.79), 'text': ' And but we saw that practically it degraded the quality not so much, so actually the quality'}, {'timestamp': (5543.79, 5544.91), 'text': ' was not bad.'}, {'timestamp': (5544.91, 5547.11), 'text': ' And this was in this paper.'}, {'timestamp': (5547.11, 5557.18), 'text': ' So they show that the quality degradation was very little, so from 26.7 to 26.5, but the performance gain were very important.'}, {'timestamp': (5557.18, 5567.38), 'text': ' We went from 48 microsecond per token to 5 microsecond or 6 microsecond per token, so a lot faster.'}, {'timestamp': (5567.38, 5574.5), 'text': " Now let's introduce the grouped query attention or the grouped multiguary attention."}, {'timestamp': (5574.5, 5581.03), 'text': ' In the multi-head attention we had n heads for the queries, n heads for the keys and'}, {'timestamp': (5581.03, 5583.03), 'text': ' n heads for the values.'}, {'timestamp': (5583.03, 5589.47), 'text': ' In the multi-query attention we have n heads for the keys but only one head for the keys'}, {'timestamp': (5589.47, 5591.64), 'text': ' and the values. In the grouped'}, {'timestamp': (5591.64, 5598.44), 'text': ' multi-query attention or the grouped query attention we have less number of'}, {'timestamp': (5598.44, 5605.3), 'text': ' heads for the keys and values so every two head for the queries in in this case for example, we will have one'}, {'timestamp': (5605.3, 5609.4), 'text': ' head for the keys and the values.'}, {'timestamp': (5609.4, 5618.75), 'text': ' And this is a good balance between quality and speed, because of course the fastest one is this one, because you have less heads, but of course the fastest one is this one, because you have less heads.'}, {'timestamp': (5618.75, 5624.15), 'text': ' But of course the best one from a quality point of view is this one, but this is a good compromise'}, {'timestamp': (5624.15, 5628.87), 'text': " between the two so you don't lose quality, but at the same time you also optimize the"}, {'timestamp': (5628.87, 5631.92), 'text': ' speed compared to the multi head attention.'}, {'timestamp': (5631.92, 5635.32), 'text': " So now that we have reviewed all this concept let's go build it."}, {'timestamp': (5635.32, 5640.94), 'text': " So please again if you didn't understand very much in detail this part is better you"}, {'timestamp': (5640.94, 5643.9), 'text': ' will review my other video about Llama in which I explain'}, {'timestamp': (5643.9, 5646.02), 'text': ' all this part much better.'}, {'timestamp': (5646.02, 5650.62), 'text': ' Otherwise if I have to repeat the same content of the previous video, the current video would'}, {'timestamp': (5650.62, 5652.74), 'text': ' become 10 hours.'}, {'timestamp': (5652.74, 5678.36), 'text': " So let's go build it. And it... Okay, we need to save some things."}, {'timestamp': (5678.36, 5684.58), 'text': ' Compared to the original code from Lama, from Facebook, from Meta, I actually removed the parallelization.'}, {'timestamp': (5684.58, 5686.3), 'text': ' First of all, because I cannot test it,'}, {'timestamp': (5686.3, 5687.74), 'text': " I don't have multiple GPUs,"}, {'timestamp': (5687.74, 5690.62), 'text': " I don't have a very powerful GPU actually."}, {'timestamp': (5690.62, 5693.82), 'text': ' And so I simplified the code a lot.'}, {'timestamp': (5696.34, 5717.88), 'text': ' Okay. and And kvHeads indicates the number of heads for the keys and the values, because they can'}, {'timestamp': (5717.88, 5722.0), 'text': ' be different than the number of heads for the queries.'}, {'timestamp': (5722.0, 5754.96), 'text': ' And this is why we also have an heads queue. This value here represents the ratio between the number of heads for the query and the'}, {'timestamp': (5754.96, 5757.76), 'text': ' number of heads for the keys and the values.'}, {'timestamp': (5757.76, 5759.96), 'text': ' We will use it later when we calculate the attention.'}, {'timestamp': (5759.96, 5762.24), 'text': ' So let me write some comments.'}, {'timestamp': (5762.24, 5764.22), 'text': ' So this is... So let me write some comments.'}, {'timestamp': (5764.22, 5765.22), 'text': ' So this is...'}, {'timestamp': (5765.22, 5766.22), 'text': ' So let me write some comments.'}, {'timestamp': (5766.22, 5767.22), 'text': ' So this is...'}, {'timestamp': (5767.22, 5768.22), 'text': ' So let me write some comments.'}, {'timestamp': (5768.22, 5769.22), 'text': ' So this is...'}, {'timestamp': (5769.22, 5770.22), 'text': ' So let me write some comments.'}, {'timestamp': (5770.22, 5771.22), 'text': ' So this is...'}, {'timestamp': (5771.22, 5772.22), 'text': ' So let me write some comments.'}, {'timestamp': (5772.22, 5773.22), 'text': ' So this is...'}, {'timestamp': (5773.22, 5774.22), 'text': ' So let me write some comments.'}, {'timestamp': (5774.22, 5775.22), 'text': ' So this is...'}, {'timestamp': (5775.22, 5776.22), 'text': ' So let me write some comments.'}, {'timestamp': (5776.22, 5815.26), 'text': ' So this is... So let me write some comments. So this is... So let me write some comments. So this is... So let me write some comments. So this is... So let me write some comments. So this is... So let me write some comments. So this is... So let me write some comments. So this is... වවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවව� And then we have a cell.headDimension, which is...'}, {'timestamp': (5815.26, 5820.41), 'text': ' This indicates the part of the embedding that will be visualized by each head, because as'}, {'timestamp': (5820.41, 5825.43), 'text': ' you know, the embedding is split into multiple heads, so each head will watch the full sentence'}, {'timestamp': (5825.43, 5837.68), 'text': ' but a part of the embedding of each word.'}, {'timestamp': (5842.98, 5855.62), 'text': ' Then we have the W matrices WQ, WK, WV and WO just like in the normal vanilla transformer.'}, {'timestamp': (5855.62, 5911.13), 'text': " And they don't have any bias. Und sie haben keine Bias. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� Oops, why did I write true? And then we create a cache."}, {'timestamp': (5911.13, 5912.73), 'text': " We will see later how it's used."}, {'timestamp': (5912.73, 5959.73), 'text': ' I just now created one for the keys and one for the values. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� Okay, finally we implement the forward method which is the salient part here.'}, {'timestamp': (5959.73, 5974.46), 'text': ' So self x is... To simplify the code for you, I will write the dimensions of the tensor that is involved'}, {'timestamp': (5974.46, 5983.35), 'text': ' in the operation and also the resulting tensor from each operation.'}, {'timestamp': (5983.35, 5988.59), 'text': ' This start position indicates just the position of the token inside of the sentence and these'}, {'timestamp': (5988.59, 5992.69), 'text': ' are the frequencies that we have computed.'}, {'timestamp': (5995.69, 6023.83), 'text': " Okay, let's see. and dimension but the sequence length we know it's one so dimension yeah then"}, {'timestamp': (6023.83, 6027.67), 'text': ' what we do is we multiply just like in the original transformer we take the'}, {'timestamp': (6027.67, 6032.45), 'text': ' query the key and values you multiply it by then the Wq Wq and Wq matrix'}, {'timestamp': (6032.85, 6036.25), 'text': ' So Xq is equal to self dot Wq'}, {'timestamp': (6039.53, 6050.9), 'text': ' This means going from B1 dimension to b1 head dimension so the number of heads for the'}, {'timestamp': (6050.9, 6061.11), 'text': ' query multiplied by the dimension because we are in this case we are this is actually equal to dim so the number of heads'}, {'timestamp': (6061.11, 6066.91), 'text': ' multiply the head dimension as you can see from here so we are not changing the'}, {'timestamp': (6066.91, 6091.22), 'text': ' the shape. In this case however, we may change the shape of the, because the number of heads for the'}, {'timestamp': (6091.22, 6094.38), 'text': ' KV may be smaller than Q.'}, {'timestamp': (6094.38, 6102.79), 'text': ' So this matrix may have a last dimension that is smaller than xq and the same is for xp.'}, {'timestamp': (6102.79, 6120.65), 'text': ' So here let me write some comment apply the WQ, WK and WV matrix to queries, keys and'}, {'timestamp': (6120.65, 6126.28), 'text': " values which are the same because it's a self-attention so the query key"}, {'timestamp': (6126.28, 6133.64), 'text': ' and value is always x. We then divide them into their corresponding number of'}, {'timestamp': (6133.64, 6139.83), 'text': ' heads so xq is equal to xq.view.'}, {'timestamp': (6139.83, 6142.67), 'text': ' Bet size we keep it like this.'}, {'timestamp': (6142.67, 6171.5), 'text': ' Sequence length is 1. So we divide B1HQ multiplied by head dimension into B1HQ and had dimension so we divide them into the h heads for the query'}, {'timestamp': (6171.5, 6237.09), 'text': ' and then we do the same for the key and the B. වවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවවව� Now, we have multiplied, okay we have the x input, we multiplied by the wq, wk and wky.'}, {'timestamp': (6237.09, 6240.29), 'text': " Let's go check the code here."}, {'timestamp': (6240.29, 6246.9), 'text': ' As you remember we take the input, we multiplied by wq, wkqwkw this will result in these matrices here'}, {'timestamp': (6246.9, 6251.7), 'text': ' we then divide them into the number of heads but in the case of grouped query attention they may'}, {'timestamp': (6251.7, 6257.75), 'text': ' be different so this may be four heads and this may be two heads and this may be two heads so they are not'}, {'timestamp': (6257.75, 6259.27), 'text': ' the same number.'}, {'timestamp': (6259.27, 6266.91), 'text': ' The next thing we are going to do and this is present in the here.'}, {'timestamp': (6266.91, 6272.93), 'text': " We need to apply the rotary positional encodings to the query and the keys, but not the values. Let's do it."}, {'timestamp': (6287.02, 6291.94), 'text': ' And this is how we apply the the original encodings. This will not change the size of the vectors.'}, {'timestamp': (6291.94, 6295.98), 'text': ' You can see that here because at the end we have the same shape'}, {'timestamp': (6295.98, 6331.34), 'text': ' as the original input vector. Okay. Now comes the KV cache part.'}, {'timestamp': (6331.34, 6341.71), 'text': " Let's watch again these lights. As you can see here, every time we have an input output token so for example the attention"}, {'timestamp': (6341.71, 6347.83), 'text': " to here it's supposed to be the token number 2 we append it at the end of the keys and"}, {'timestamp': (6347.83, 6352.37), 'text': ' the values and this is exactly what we are going to do.'}, {'timestamp': (6352.37, 6358.17), 'text': ' So what we do here, we keep a cache of the keys and the values because they will be used'}, {'timestamp': (6358.17, 6366.74), 'text': ' for the next iterations because at every iteration in X we only receive the latest token that was output from the previous'}, {'timestamp': (6366.74, 6375.7), 'text': ' iteration. We append it to the K and the V and then we compute the attention between all the K,'}, {'timestamp': (6375.7, 6378.95), 'text': ' all the V but only the single token as query.'}, {'timestamp': (6381.35, 6400.65), 'text': " Let's do it. So first replace. This is the position of the torque in."}, {'timestamp': (6400.65, 6404.22), 'text': ' This should be one because sequence length is actually one always'}, {'timestamp': (6405.38, 6408.7), 'text': ' But I try to keep this code the same as the one from'}, {'timestamp': (6409.58, 6411.18), 'text': ' llama'}, {'timestamp': (6411.18, 6413.18), 'text': ' from meta'}, {'timestamp': (6416.66, 6423.27), 'text': ' This is basically it means that if we have one token from many batches, I mean we have one token'}, {'timestamp': (6423.27, 6431.05), 'text': ' for every batch, we replace them because we can process multiple batches.'}, {'timestamp': (6431.05, 6454.06), 'text': ' So we replace the entry for this position here, but when we compute the attention using'}, {'timestamp': (6454.06, 6457.35), 'text': " the KV cache, Let's go watch again."}, {'timestamp': (6457.35, 6466.15), 'text': ' We need to calculate the product, the dot product, between the only one token but all'}, {'timestamp': (6466.15, 6472.29), 'text': ' the keys and then we need to multiply with all the values and this will result in only one token as output'}, {'timestamp': (6472.29, 6480.61), 'text': ' So we need to extract from this cache all the tokens as keys and all the tokens as values up to this position here'}, {'timestamp': (6481.09, 6484.82), 'text': ' The one we are passing so'}, {'timestamp': (6491.82, 6501.27), 'text': ' this is equal to to all.'}, {'timestamp': (6501.27, 6527.02), 'text': ' So starting from zero up to start pose plus sequence length and the values are... length'}, {'timestamp': (6527.02, 6529.86), 'text': ' now'}, {'timestamp': (6529.86, 6532.94), 'text': ' now what happens is that'}, {'timestamp': (6532.94, 6542.71), 'text': ' let me write also some sizes here we have b sequence length of k and v because the'}, {'timestamp': (6542.71, 6547.03), 'text': ' sequence length of the input is always one we know that but the sequence length'}, {'timestamp': (6547.03, 6555.61), 'text': ' of the cache means all the all the cached keys and values which are up to'}, {'timestamp': (6555.61, 6556.61), 'text': ' start position.'}, {'timestamp': (6556.61, 6562.17), 'text': ' So this sequence length is actually equal to start position.'}, {'timestamp': (6562.17, 6567.02), 'text': ' And actually start position plus one'}, {'timestamp': (6567.02, 6575.02), 'text': ' next dimension is the number of heads for the key and v and then the dimension of each head'}, {'timestamp': (6575.02, 6577.59), 'text': ' now the number of each head.'}, {'timestamp': (6577.59, 6584.15), 'text': ' Now the number of heads for the keys and values may not correspond to the number of heads'}, {'timestamp': (6584.15, 6586.67), 'text': ' of the queries.'}, {'timestamp': (6586.67, 6588.31), 'text': ' So how do we compute?'}, {'timestamp': (6588.31, 6603.8), 'text': " In the original code from Llama what they did was basically, let's check So how do we compute? number of heads for the keys and the values is not the same as the number of heads for"}, {'timestamp': (6603.8, 6604.8), 'text': ' the queries.'}, {'timestamp': (6604.8, 6606.52), 'text': ' So, there are two ways.'}, {'timestamp': (6606.52, 6610.48), 'text': ' One is to make an optimized algorithm that actually takes this into consideration.'}, {'timestamp': (6610.48, 6622.03), 'text': ' The other way is to just copy this single head into multiple heads such that we arrive to this situation here and then we'}, {'timestamp': (6622.03, 6624.67), 'text': ' just compute it just like a multi-head.'}, {'timestamp': (6624.67, 6629.63), 'text': ' This is not an optimized solution but is the one used by the code by Lama.'}, {'timestamp': (6629.63, 6645.38), 'text': " And it's also the one I will be code by Lamao, so with 70 billion parameters, but my computer"}, {'timestamp': (6645.38, 6648.02), 'text': ' will never be able to load that model.'}, {'timestamp': (6648.02, 6652.1), 'text': " And so I don't have any way of testing it, so that's why I also didn't optimize the"}, {'timestamp': (6652.1, 6656.3), 'text': ' code for actually computing the grouped query attention, but I will just replicate this'}, {'timestamp': (6656.3, 6657.79), 'text': ' single head multiple times'}, {'timestamp': (6657.79, 6661.83), 'text': ' such that we arrive to this situation here.'}, {'timestamp': (6661.83, 6702.79), 'text': ' So I will also repeat. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� Okay, this function here, repeat kv just repeats the the keys until we reach the'}, {'timestamp': (6702.79, 6708.11), 'text': ' number of for this number of times so n-wrap what is this is the ratio of the'}, {'timestamp': (6708.11, 6711.97), 'text': ' number of heads of the queries by the number of heads of the keys so if the number of heads of the queries by the number of heads of the keys.'}, {'timestamp': (6711.97, 6716.97), 'text': ' So if the number of heads of the keys is 4 and the number of heads for the queries is'}, {'timestamp': (6716.97, 6721.21), 'text': ' 8, we need to repeat twice each head.'}, {'timestamp': (6721.21, 6753.69), 'text': " So let's build also this method since we are here. Okay, we don't need to repeat it, so there is only one repetition, we just return the"}, {'timestamp': (6753.69, 6754.69), 'text': ' basic.'}, {'timestamp': (6754.69, 6760.01), 'text': ' The answer, otherwise we repeat it, end times.'}, {'timestamp': (6760.01, 6780.35), 'text': ' So the first thing we do is we add a new dimension. And we can do like this.'}, {'timestamp': (6780.35, 6789.15), 'text': ' About super length, number of heads, then nothing.'}, {'timestamp': (6789.15, 6794.81), 'text': ' And then this will add this new dimension in this position'}, {'timestamp': (6794.81, 6828.95), 'text': ' then we reshape it.'}, {'timestamp': (6835.61, 6841.77), 'text': ' Basically we introduce a new dimension, we repeat this all the sequence this dimension number of times, so and wrap along this dimension and wrap number of times, and then we just'}, {'timestamp': (6841.77, 6850.42), 'text': ' flatten it. So we remove again this dimension and this is how we repeat'}, {'timestamp': (6850.42, 6860.35), 'text': ' these keys and also the values. Now we can repeat.'}, {'timestamp': (6866.35, 6874.73), 'text': ' Okay, now we just proceed just like with the standard calculation for the multi-head attention.'}, {'timestamp': (6874.73, 6886.74), 'text': ' That is, we first move the head dimension before the sequence dimension because each head will watch all the sequence but a part of the embedding'}, {'timestamp': (6886.74, 6892.1), 'text': ' of each token.'}, {'timestamp': (6892.1, 6901.43), 'text': ' So what we are doing is batch one because one is the sequence length of the queries, the number of heads of the queries,'}, {'timestamp': (6901.43, 6903.23), 'text': ' and head dimension.'}, {'timestamp': (6903.23, 6913.37), 'text': ' Batch, head, sequence length length and head dimension.'}, {'timestamp': (6913.37, 6934.7), 'text': ' We do the same for for queries multiplied by the transpose of the keys divided'}, {'timestamp': (6934.7, 6940.36), 'text': ' by the square root of the dimension of each head.'}, {'timestamp': (6940.36, 6957.69), 'text': ' So, xq, so the query is multiplied by the transpose of the keys. All of this divided by the square root of the dimension of each head. Then we'}, {'timestamp': (6957.69, 6993.69), 'text': ' apply the softmax. and this one will result in a shape of queries one add dimension multiplied by QV.'}, {'timestamp': (6993.69, 6999.17), 'text': " The softmax doesn't change the dimension."}, {'timestamp': (6999.17, 7020.36), 'text': ' Then we multiply it by the values.'}, {'timestamp': (7020.36, 7055.62), 'text': ' So this will result in mb Merci. and then we multiplied by the output matrix but before we remove all the heads so we concatenate'}, {'timestamp': (7055.62, 7060.08), 'text': ' again this is what we did also here. So here'}, {'timestamp': (7060.08, 7064.6), 'text': ' we take the output of all the heads, then we concatenate them together and then we multiply'}, {'timestamp': (7064.6, 7122.81), 'text': ' it by the WO matrix. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� and this will result in a D1 dim, b1 dim. This one is bHQ1 head dimension into b1HQ head dimension because of the transposition and then we remove the'}, {'timestamp': (7122.81, 7130.26), 'text': ' dimension for the head so B1 dimension and this is our self-attention with'}, {'timestamp': (7130.26, 7138.96), 'text': " KV cache so let's review what we have done here I think I made some mistake because of self."}, {'timestamp': (7138.96, 7140.48), 'text': " That's why it's color different."}, {'timestamp': (7140.48, 7143.24), 'text': " Okay, let's review what we have done."}, {'timestamp': (7143.24, 7147.0), 'text': ' When we calculate the self-attention because we are inferencing, so this code will only'}, {'timestamp': (7147.0, 7153.25), 'text': ' work for inferencing, we can use the KVcache. The KVcache allows us to save a number of dot'}, {'timestamp': (7153.25, 7158.93), 'text': " products that we don't need. Why? Because every time we are computing, in the original transformer,"}, {'timestamp': (7158.93, 7165.72), 'text': " we were computing a lot of dot products for tokens, output tokens that we don't care about."}, {'timestamp': (7165.72, 7169.88), 'text': ' In this case, we simplified the mechanism to output only one token.'}, {'timestamp': (7169.88, 7176.6), 'text': ' As you can see, the output of the self-attention is B, so batch, one token only with its embedding'}, {'timestamp': (7176.6, 7179.08), 'text': ' size, which is 4096'}, {'timestamp': (7180.0, 7184.72), 'text': ' So we are only outputting one token not many tokens'}, {'timestamp': (7184.72, 7188.0), 'text': ' We input only one token and we output one token and'}, {'timestamp': (7188.84, 7193.61), 'text': ' But because we need to relate that single token with all the previous tokens, we keep'}, {'timestamp': (7193.61, 7196.65), 'text': ' a cache of the keys and the values.'}, {'timestamp': (7196.65, 7201.65), 'text': ' Every time we have a token, we put it into the cache, like here, then we retrieve all'}, {'timestamp': (7201.65, 7205.46), 'text': ' the previous saved tokens from the cache, and then we calculate'}, {'timestamp': (7205.46, 7210.18), 'text': ' the attention between all the previous tokens, so the keys and the values, and the single'}, {'timestamp': (7210.18, 7214.06), 'text': ' token as input of as queries.'}, {'timestamp': (7214.06, 7216.84), 'text': ' The output is the only token we care about.'}, {'timestamp': (7216.84, 7221.28), 'text': ' This is the idea behind KBcache.'}, {'timestamp': (7221.28, 7226.64), 'text': ' And the grouped query attention is the fact that we have different number of heads for'}, {'timestamp': (7226.64, 7229.0), 'text': ' the keys and values.'}, {'timestamp': (7229.0, 7235.13), 'text': ' But in our case, we do have different number of heads for the keys'}, {'timestamp': (7235.13, 7242.37), 'text': ' and queries, but we just repeat the one that we are missing to calculate the attention.'}, {'timestamp': (7242.37, 7248.14), 'text': ' So the attention is calculated just like the previous transformer, like a normal multi-head'}, {'timestamp': (7248.14, 7254.98), 'text': ' attention, but by repeating the missing keys and values heads instead of actually optimizing'}, {'timestamp': (7254.98, 7255.98), 'text': ' the algorithm.'}, {'timestamp': (7255.98, 7260.12), 'text': ' This has also been done by Meta in its official implementation'}, {'timestamp': (7260.12, 7261.52), 'text': ' and I also did it here.'}, {'timestamp': (7261.52, 7266.48), 'text': ' The biggest reason is because I cannot test any other modification. I cannot test the'}, {'timestamp': (7267.2, 7270.69), 'text': ' another algorithm that actually tries to optimize this calculation.'}, {'timestamp': (7270.69, 7276.69), 'text': ' So if I find another implementation that I know is working, I will share it with you guys.'}, {'timestamp': (7276.69, 7281.69), 'text': ' Otherwise, I will try to run it on Colab and see if I can come up with a better solution.'}, {'timestamp': (7281.69, 7298.16), 'text': " But for now, we just repeat it. But at least we got the concept of the group. I can come up with a better solution. and the multi-head attention that doesn't sacrifice quality but improves speed."}, {'timestamp': (7299.28, 7304.32), 'text': " Now the last thing that we didn't implement is the feed forward layer. For the feed forward"}, {'timestamp': (7304.32, 7308.4), 'text': ' layer the only thing that we need to review is the ZWIGGLUE activation function that we can see here.'}, {'timestamp': (7309.52, 7315.09), 'text': ' And this activation function has been changed compared to the previous activation function'}, {'timestamp': (7315.09, 7318.65), 'text': ' used in the vanilla transformer which was the relu function.'}, {'timestamp': (7318.65, 7322.93), 'text': ' And the only reason we replaced it is because this one performs better.'}, {'timestamp': (7322.93, 7328.22), 'text': ' And as I showed in my previous video, there is no hope say we cannot prove why it works'}, {'timestamp': (7328.22, 7332.82), 'text': " better because in such a big model with 70 billion parameters it's difficult to explain"}, {'timestamp': (7332.82, 7336.1), 'text': ' why something a little modification works better than another.'}, {'timestamp': (7336.1, 7339.88), 'text': ' We just know that some things that work better in practice for that kind of model or for'}, {'timestamp': (7339.88, 7341.4), 'text': ' that kind of application.'}, {'timestamp': (7341.4, 7344.76), 'text': ' And this is actually not my opinion, this is actually written in the paper.'}, {'timestamp': (7344.76, 7349.64), 'text': ' So as you can see here in the conclusion of the paper, they say that we offer no explanation'}, {'timestamp': (7349.64, 7350.29), 'text': ' as to why these'}, {'timestamp': (7350.29, 7354.85), 'text': ' architectures seem to work. We attribute their success as all else to divine benevolence.'}, {'timestamp': (7354.85, 7359.25), 'text': ' So it means that when you have such a big model and you change a little thing and it works better,'}, {'timestamp': (7359.25, 7363.46), 'text': ' you cannot always come up with a pattern to describe why it is working better.'}, {'timestamp': (7363.46, 7368.94), 'text': ' You just take it for granted that it works better and you use it because it works better'}, {'timestamp': (7368.94, 7371.1), 'text': ' in practice.'}, {'timestamp': (7371.1, 7376.3), 'text': ' So to implement this VGLO function we need to apply this is the formula from the original'}, {'timestamp': (7376.3, 7380.04), 'text': ' transformer. So then we have'}, {'timestamp': (7380.04, 7384.4), 'text': ' two matrices here so this is the ReLU function of the first linear layer and'}, {'timestamp': (7384.4, 7389.44), 'text': ' the second linear layer in the Lama we use the ZWIGLU function which involves'}, {'timestamp': (7389.44, 7393.73), 'text': ' the three matrices here because they incremented the number of parameters here'}, {'timestamp': (7393.73, 7396.05), 'text': ' and also they were experimenting with the'}, {'timestamp': (7396.77, 7398.45), 'text': ' grouped query attention'}, {'timestamp': (7398.45, 7401.01), 'text': ' the architecture of Lama has a'}, {'timestamp': (7401.97, 7407.3), 'text': ' has some more parameters to adjust the number of parameters of this feed forward'}, {'timestamp': (7407.3, 7411.1), 'text': ' layer so as it respects some constraints.'}, {'timestamp': (7411.1, 7419.68), 'text': ' And this is actually used in deep learning research whenever we modify the transformer model and this reduces the number of parameters'}, {'timestamp': (7419.68, 7423.92), 'text': ' or increases the number of parameters, the first thing the researchers do, they adjust'}, {'timestamp': (7423.92, 7429.36), 'text': ' the numbers of parameters of the feed forward layer so that when they make comparison between'}, {'timestamp': (7429.36, 7432.29), 'text': ' two models, they have the same number of parameters.'}, {'timestamp': (7432.29, 7438.17), 'text': ' So I will also of course use the same structure because otherwise I cannot load the weight'}, {'timestamp': (7438.17, 7441.77), 'text': ' from the pre-trained model.'}, {'timestamp': (7441.77, 7444.46), 'text': " So let's do it."}, {'timestamp': (7444.46, 7449.8), 'text': ' The hidden size is calculated like this, so 4 times the dimension, then they do the 2'}, {'timestamp': (7449.8, 7499.2), 'text': " thirds of this dimension. And then they also have a multiplier if it's specified. Then they say calculate the hidden dimension like this it may not be"}, {'timestamp': (7499.2, 7504.32), 'text': ' the case that this hidden dimension is a multiple of this number here so maybe'}, {'timestamp': (7504.32, 7509.48), 'text': ' they want the size of the hidden layer to be a multiple of 256 part by'}, {'timestamp': (7509.48, 7512.93), 'text': ' calculating it like this it may not be so what they do is they'}, {'timestamp': (7512.93, 7537.56), 'text': ' make it round up to the next multiple of the multiple of parameter This is a way to do it.'}, {'timestamp': (7537.56, 7539.0), 'text': ' Ok let me give you an example.'}, {'timestamp': (7539.0, 7542.68), 'text': " It's easy to show with an example than to actually write it."}, {'timestamp': (7542.68, 7553.17), 'text': " So suppose you have a hidden size is equal to let's say 7 but you want it to multiple of is equal to 5 so you want the"}, {'timestamp': (7553.17, 7560.17), 'text': ' hidden size to be a multiple of 5 so how do we do well what we do is basically we do hidden'}, {'timestamp': (7560.17, 7575.5), 'text': ' plus 4 in this case so we do 7 plus 5 which is equal to 2 and then we multiply this 2 by 5.'}, {'timestamp': (7575.5, 7579.28), 'text': ' So it will result in 2 by 5 is equal to 10 it will result in'}, {'timestamp': (7579.28, 7594.69), 'text': " the first multiple that is bigger or equal to this number here that's the idea then we have these matrices for this VWIGLU function. It's very easy. We just follow the formula for"}, {'timestamp': (7594.69, 7607.34), 'text': ' the ZWIGLU function which is here. So W is wish of what is wish? This switch is the silo function because the switch'}, {'timestamp': (7607.34, 7611.74), 'text': ' with beta is equal to one is actually the silo function which has this graph'}, {'timestamp': (7611.74, 7618.74), 'text': ' here and then we multiply it with another parameter matrix here and then we apply it to another'}, {'timestamp': (7618.74, 7620.3), 'text': ' linear layer W2.'}, {'timestamp': (7620.3, 7671.69), 'text': " So in total we have three matrices W1, we call it W2 and W3. And they don't have bias. This is the forward method"}, {'timestamp': (7677.69, 7695.82), 'text': ' the first thing we do is we calculate the switch function. Then we calculate, so we are calculating, let me show you, we are calculating'}, {'timestamp': (7695.82, 7716.97), 'text': ' this one xw switch of xw then we calculate this xv then we multiply them together just like in the formula so we should multiply by xv and'}, {'timestamp': (7716.97, 7731.22), 'text': ' then we apply the last linear layer which is w2 which results in a multiplication by the W2 matrix by the way and then return X and'}, {'timestamp': (7731.22, 7741.52), 'text': ' this is the field forward layer. Now that we have all the building blocks, we need to go to the inferencing.'}, {'timestamp': (7741.52, 7744.36), 'text': " Let's start building the inference code."}, {'timestamp': (7744.36, 7747.64), 'text': ' So inference.py.'}, {'timestamp': (7747.64, 7753.09), 'text': ' The first code, first we will build the code to load the model and then we will build the code to'}, {'timestamp': (7753.09, 7758.77), 'text': ' inference the model. I will actually also show all the inference techniques that are out there'}, {'timestamp': (7758.77, 7767.08), 'text': " and which one we will apply and why. So let's start by building first the code for loading the model. So let's"}, {'timestamp': (7767.08, 7788.48), 'text': ' import the stuff we need the sentence piece to load the'}, {'timestamp': (7788.48, 7792.05), 'text': ' tokenizer because the sentence piece is the tokenizer that has'}, {'timestamp': (7792.05, 7807.34), 'text': " been used and it's a library from Google. Okay, from model import, model args and the transformer class."}, {'timestamp': (7807.34, 7823.36), 'text': ' We define the class Lama, which is a sentence piece processor, and then the model'}, {'timestamp': (7823.36, 7845.02), 'text': ' arguments. Ops! Oops, RX.'}, {'timestamp': (7845.02, 7846.02), 'text': ' Oh, I missed it.'}, {'timestamp': (7846.02, 7847.02), 'text': ' Model RX.'}, {'timestamp': (7847.02, 7850.02), 'text': ' Yeah, Model RX.'}, {'timestamp': (7850.02, 7851.02), 'text': ' Okay.'}, {'timestamp': (7851.02, 7855.02), 'text': ' Now, we build a static method.'}, {'timestamp': (7855.02, 7859.12), 'text': ' Static method and we call it build, just like in the original code from llama'}, {'timestamp': (7861.04, 7866.96), 'text': ' in which we pass the directory where the checkpoints are saved in this case the directory'}, {'timestamp': (7866.96, 7871.89), 'text': ' name is llama27b in my case but it depends on which size of the model you have'}, {'timestamp': (7871.89, 7872.89), 'text': ' downloaded.'}, {'timestamp': (7872.89, 7877.85), 'text': ' Then the tokenizer path, which is the path to the tokenizer.'}, {'timestamp': (7877.85, 7881.93), 'text': ' This is the file of the tokenizer that I downloaded.'}, {'timestamp': (7881.93, 7885.02), 'text': ' Then we have load model,'}, {'timestamp': (7885.02, 7889.02), 'text': ' play and mark sequence length,'}, {'timestamp': (7889.02, 7894.02), 'text': ' mark patch size,'}, {'timestamp': (7894.02, 7900.36), 'text': ' and there we have device.'}, {'timestamp': (7901.74, 7907.74), 'text': ' Okay this is only for displaying how much time it takes to load the model.'}, {'timestamp': (7907.74, 7913.69), 'text': ' If we want to load the model we will also load the checkpoints.'}, {'timestamp': (7913.69, 7921.69), 'text': ' So checkpoints is equal to sorted.'}, {'timestamp': (7921.69, 7962.25), 'text': ' The global method allows you to find all the files that we are loading checkpoint this one.'}, {'timestamp': (7962.25, 7965.02), 'text': ' And then we actually load it'}, {'timestamp': (7970.02, 7975.02), 'text': ' and we save it on the CPU'}, {'timestamp': (7981.44, 8008.33), 'text': ' we can show how much time it takes to load all'}, {'timestamp': (8008.33, 8009.73), 'text': ' the parameters of the model.'}, {'timestamp': (8009.73, 8036.69), 'text': ' Then we load the parametersIt has read only file.'}, {'timestamp': (8063.36, 8075.13), 'text': ' Then we build the max patch size is the max patch size the device is the one we have specified and then all the parameters loaded from the JSON file. Then we loaded the'}, {'timestamp': (8075.13, 8091.07), 'text': ' tokenizer. Then we by using the tokenizer we can populate the vocab size of the model args.'}, {'timestamp': (8091.07, 8112.79), 'text': ' The vocabulary size is actually not the number of tokens inside the tokenizer. tensor for PyTorch.'}, {'timestamp': (8112.79, 8118.07), 'text': " So whenever PyTorch wants to create a new tensor, what kind of type it should use, it's defined"}, {'timestamp': (8118.07, 8119.19), 'text': ' this is by meta.'}, {'timestamp': (8119.19, 8131.39), 'text': ' So they want for CUDA to use this type that I show you here default tensor type torch dot CUDA half tensor this'}, {'timestamp': (8131.39, 8150.0), 'text': ' changes the precision that the tensor supports so how much space it occupies in memory.'}, {'timestamp': (8150.0, 8172.45), 'text': ' Otherwise. Then we created the actual model. Ok, when we load a checkpoint, actually the checkpoint is a list of key and values.'}, {'timestamp': (8172.45, 8175.87), 'text': ' Each key is a matrix in the model.'}, {'timestamp': (8175.87, 8180.18), 'text': ' So the weight for example of a linear layer or the bias of a linear layer or something'}, {'timestamp': (8180.18, 8181.68), 'text': ' like this.'}, {'timestamp': (8181.68, 8193.47), 'text': ' And the names that we have used for the variable names and the matrices here, for example, WQQWK match actually the name that are present'}, {'timestamp': (8193.47, 8197.03), 'text': ' in the checkpoint here except for one name.'}, {'timestamp': (8197.03, 8205.63), 'text': ' So to make sure that I have used the right names I will load the checkpoint with strict equal true.'}, {'timestamp': (8205.63, 8209.75), 'text': " Strict equal true means that if there is at least one name that doesn't match it will"}, {'timestamp': (8209.75, 8210.75), 'text': ' throw an error.'}, {'timestamp': (8210.75, 8223.76), 'text': ' So if load model, model.loadStateDict strict equal true.'}, {'timestamp': (8223.76, 8229.16), 'text': " So if there is at least one name in the loaded file that doesn't match the name in the classes"}, {'timestamp': (8229.16, 8232.29), 'text': ' that I have created here in the module it will throw an error.'}, {'timestamp': (8232.29, 8238.13), 'text': " But I know that there is one key that we don't need which are the frequencies for the rotary"}, {'timestamp': (8238.13, 8244.51), 'text': ' positional embeddings which we actually are computing every time we create the tensor so we are creating them'}, {'timestamp': (8245.47, 8251.31), 'text': " here by using this function so we don't need to load them from the model so we can remove it from"}, {'timestamp': (8251.31, 8258.88), 'text': " the model from the checkpoint so because the checkpoint is a dictionary we can just remove this it's"}, {'timestamp': (8258.88, 8264.88), 'text': ' called rope dot frex and then we can print how much time it took to load the'}, {'timestamp': (8264.88, 8290.67), 'text': ' model and then return the Lama, Model, Tokenizer and ModelArgs.'}, {'timestamp': (8290.67, 8312.69), 'text': ' Now before we proceed further, let me test if the model can be successfully loaded. CD to 0 so later we use it for inferencing.'}, {'timestamp': (8312.69, 8316.69), 'text': " Then I don't want to use CUDA because my GPU doesn't support it so I say allow CUDA"}, {'timestamp': (8316.69, 8327.03), 'text': ' is equal to 4 then device is equal to storage.cuda.isavailable and allow kuda.'}, {'timestamp': (8327.03, 8333.27), 'text': ' Next time if you want to load the model with kuda, I just set this variable to true, but'}, {'timestamp': (8333.27, 8375.59), 'text': " in my case I will always leave it to false because I don't want to load CUDA. So you can slang this to 1024, max-patch size let's say 3 and device."}, {'timestamp': (8375.59, 8389.56), 'text': " Now. Now, let's run it and hopefully it will not crash."}, {'timestamp': (8389.56, 8404.35), 'text': ' Wow, already. will not crash. There is always a lot of typos when you write code'}, {'timestamp': (8406.35, 8407.71), 'text': ' Another problem here'}, {'timestamp': (8409.71, 8410.35), 'text': ' Not storage but'}, {'timestamp': (8412.35, 8413.11), 'text': ' tensor, this should be'}, {'timestamp': (8415.11, 8415.33), 'text': ' tensor'}, {'timestamp': (8420.36, 8426.0), 'text': " BFLOAD16 tensor, yeah, let's try again. Hidden, hidden what?"}, {'timestamp': (8426.0, 8442.17), 'text': ' Hidden dimension of course.'}, {'timestamp': (8442.17, 8445.03), 'text': " Let's try again."}, {'timestamp': (8445.03, 8450.03), 'text': ' Yeah, all OK.'}, {'timestamp': (8450.03, 8452.03), 'text': ' Wonderful.'}, {'timestamp': (8452.03, 8456.92), 'text': " It means that at least it's doing something and it's not crashing, which is always a good news."}, {'timestamp': (8456.92, 8459.72), 'text': ' So our next step is actually to build the inferencing code.'}, {'timestamp': (8459.72, 8465.64), 'text': ' So what we want to do is actually we want to be able to give some prompts to the model'}, {'timestamp': (8465.64, 8467.8), 'text': ' and then check the output for this prompt.'}, {'timestamp': (8467.8, 8472.97), 'text': " So let's define some prompts. We will define some prompts"}, {'timestamp': (8472.97, 8480.89), 'text': ' here and here we pass for example the size of the prompts and then we want to'}, {'timestamp': (8480.89, 8486.39), 'text': ' you know we want to inference the model.'}, {'timestamp': (8486.39, 8490.63), 'text': ' So before we start inferencing the model, we need to build the code for inferencing'}, {'timestamp': (8490.63, 8495.35), 'text': ' the model because we need to find a strategy for selecting the next token, etc.'}, {'timestamp': (8495.35, 8498.52), 'text': " So let's review how the inferencing works and what are the various"}, {'timestamp': (8498.52, 8504.0), 'text': ' strategies for inferencing. Okay, so when we are dealing with the next token prediction'}, {'timestamp': (8504.0, 8509.04), 'text': ' task, when we want to inference, we usually give the prompt and then we want to predict'}, {'timestamp': (8509.04, 8510.27), 'text': ' the tokens.'}, {'timestamp': (8510.27, 8515.13), 'text': ' But we give one token at a time and every time we give one more token the model will'}, {'timestamp': (8515.13, 8518.73), 'text': ' output one more token as output and we only keep the last one.'}, {'timestamp': (8518.73, 8525.45), 'text': ' But with the KVcash actually we always give one token at a time, KV cash will keep the cash for the keys'}, {'timestamp': (8525.45, 8531.85), 'text': ' and the values and with only output one token. Okay the point is we need'}, {'timestamp': (8531.85, 8536.25), 'text': ' to find strategies for selecting this token among all the tokens that we have'}, {'timestamp': (8536.25, 8539.12), 'text': ' in the vocabulary and this is the job of the logits'}, {'timestamp': (8539.12, 8547.2), 'text': " and the softmax. So let's review how they work. Now imagine I give you the following task as human"}, {'timestamp': (8547.2, 8553.25), 'text': ' so complete the following sentence I think nuclear power is. And then you have'}, {'timestamp': (8553.25, 8558.53), 'text': ' to choose a word. Now, you as human may have thought of the possible next tokens, which'}, {'timestamp': (8558.53, 8564.51), 'text': ' may be clean, dangerous, cheap, expensive, safe, difficult or something else.'}, {'timestamp': (8564.51, 8569.03), 'text': ' The choice of the next token in your head depends on your education, on your experience with'}, {'timestamp': (8569.03, 8572.71), 'text': ' nuclear power and your opinion on the matter.'}, {'timestamp': (8572.71, 8575.59), 'text': ' Large language models also face the same problem.'}, {'timestamp': (8575.59, 8578.72), 'text': ' When we give them a prompt, then the model has to choose'}, {'timestamp': (8578.72, 8584.76), 'text': ' the next word. The model, the uncertainty of the choice derives entirely from their training'}, {'timestamp': (8584.76, 8591.81), 'text': ' process and the strategy that we use to select the next token. There are many strategies. For example, we have the'}, {'timestamp': (8591.81, 8596.85), 'text': ' greedy strategy, the beam search, temperature is a parameter, random sampling, top K, top'}, {'timestamp': (8596.85, 8602.29), 'text': ' B. In this video, we will review all these strategies and how they work. But first,'}, {'timestamp': (8602.29, 8604.51), 'text': ' we need to understand what are the logits.'}, {'timestamp': (8604.51, 8609.19), 'text': " Let's look at the transformer model from Lama."}, {'timestamp': (8609.19, 8613.99), 'text': ' So the output of the self-attention is a sequence.'}, {'timestamp': (8613.99, 8617.28), 'text': " In the case of the KVcache, it's only one token. We then run it through a linear layer, so after normalizing it, we can see that the output of the KVcache is only one token."}, {'timestamp': (8617.28, 8624.76), 'text': ' We then run it through a linear layer, the linear layer will transform the embedding'}, {'timestamp': (8624.76, 8635.39), 'text': ' that is output from the self-attention here into a list of numbers that represent the kind of the probability, they are not'}, {'timestamp': (8635.39, 8641.83), 'text': ' really a probability, but we can think of it as a probability of that token in the vocabulary.'}, {'timestamp': (8641.83, 8646.27), 'text': " So if our vocabulary is made of let's say 100 tokens,"}, {'timestamp': (8646.27, 8653.47), 'text': ' this linear layer will output 100 numbers. And after we apply the softmax, these 100 numbers'}, {'timestamp': (8653.47, 8660.36), 'text': ' will become the probability of that token being the next more probable token for the'}, {'timestamp': (8660.36, 8662.84), 'text': ' prompt given to the input.'}, {'timestamp': (8662.84, 8668.4), 'text': ' So given an input, a prompt, the model comes up with probabilities.'}, {'timestamp': (8668.4, 8686.67), 'text': ' Probabilities for which token to choose next. list of numbers such that each number represents a score that later with the Softmax represents'}, {'timestamp': (8686.67, 8691.11), 'text': ' the probability of that particular token in the vocabulary.'}, {'timestamp': (8691.11, 8695.67), 'text': ' The Softmax job is just to scale the logits in such a way that they sum up to one, so'}, {'timestamp': (8695.67, 8698.0), 'text': " that's why we can talk about probabilities with the Softmax but not with the logits in such a way that they sum up to one. So that's why we can talk about probabilities"}, {'timestamp': (8698.0, 8703.56), 'text': ' with the softmax but not with the logits. So the output of the softmax is still a probability'}, {'timestamp': (8703.56, 8709.0), 'text': ' distribution over all the words in the vocabulary. That is, each word in the vocabulary will'}, {'timestamp': (8709.0, 8710.89), 'text': ' have a probability associated to it. But now, given these words, each word in the vocabulary will have a probability associated to it.'}, {'timestamp': (8710.89, 8716.05), 'text': ' But now, given these words, with each one with their probability, how do we choose the next'}, {'timestamp': (8716.05, 8717.05), 'text': ' token?'}, {'timestamp': (8717.05, 8718.73), 'text': ' There are many strategies.'}, {'timestamp': (8718.73, 8720.69), 'text': ' The easiest one is the greedy.'}, {'timestamp': (8720.69, 8725.79), 'text': ' The greedy strategy basically says, we just select the token with the maximum probability.'}, {'timestamp': (8725.79, 8731.71), 'text': ' So, imagine we are inferencing and the first time step in the greedy strategy.'}, {'timestamp': (8731.71, 8736.11), 'text': ' The prompt is, Celia, you are breaking my heart, you are shaking my...'}, {'timestamp': (8736.11, 8750.61), 'text': ' This is a line from a very famous song from Simona. you are breaking my heart. You are shaking my confidence'}, {'timestamp': (8751.69, 8755.43), 'text': ' Suppose the output of the softmax is this distribution here'}, {'timestamp': (8755.51, 8763.35), 'text': ' So we have 40% probability for this word 20% for this word 15% for this word and 10% for this word'}, {'timestamp': (8766.45, 8771.23), 'text': ' With a gritty strategy we always choose the token with the maximum probability. Then we appended to the'}, {'timestamp': (8771.23, 8776.23), 'text': " input so the input at the next inference step becomes silly. You're breaking my"}, {'timestamp': (8776.23, 8777.72), 'text': ' heart, you are shaking my confidence'}, {'timestamp': (8777.72, 8782.08), 'text': ' and then the model has to come up with the next word, which, if you know the song, is'}, {'timestamp': (8782.08, 8783.08), 'text': ' daily.'}, {'timestamp': (8783.08, 8788.48), 'text': ' If we use the greedy strategy, we select the one with the highest probability, so in this'}, {'timestamp': (8788.48, 8791.33), 'text': " case it's daily and it's also the correct one."}, {'timestamp': (8791.33, 8793.77), 'text': ' So this is how the greedy strategy works.'}, {'timestamp': (8793.77, 8798.33), 'text': ' At every step we choose the token with the maximum probability, which is then appended'}, {'timestamp': (8798.33, 8802.01), 'text': ' to the input to generate the next token and so on.'}, {'timestamp': (8802.01, 8806.59), 'text': ' But if the initial token happens to be the wrong one, so not only the initial but the'}, {'timestamp': (8806.59, 8811.51), 'text': " initial two, three tokens happen to be the wrong ones, it's very likely that all the"}, {'timestamp': (8811.51, 8819.28), 'text': " next tokens will also be wrong because we are giving a wrong prompt to the model. So imagine at the time step one, we don't choose"}, {'timestamp': (8819.28, 8824.48), 'text': ' confidence but somehow the model came up with a high score for lever, so you are shaking'}, {'timestamp': (8824.48, 8829.5), 'text': ' my lever but then the next word, the model will not be able to come up with a reasonable'}, {'timestamp': (8829.5, 8832.05), 'text': ' next word because there is no'}, {'timestamp': (8832.05, 8834.85), 'text': ' song that says you are shaking my liver.'}, {'timestamp': (8834.85, 8839.41), 'text': ' So if we make a mistake in the early stage of the greedy, all the next token very probably'}, {'timestamp': (8839.41, 8840.85), 'text': ' will also be wrong.'}, {'timestamp': (8840.85, 8843.51), 'text': " But it's very easy to implement. And however, it performs poorly in practice that it's very easy to implement."}, {'timestamp': (8843.51, 8849.63), 'text': " And however it performs poorly in practice that it's not used so much."}, {'timestamp': (8849.63, 8851.91), 'text': ' Another strategy is the beam search.'}, {'timestamp': (8851.91, 8859.16), 'text': ' In beam search we have a parameter which is called K, which means that at every step we not only choose the top ones, but'}, {'timestamp': (8859.16, 8866.24), 'text': ' the top K at every step and we always keep the top 2 best performing tokens.'}, {'timestamp': (8866.24, 8871.29), 'text': ' So in this case for example, imagine we are time step one, so Celia you are breaking my heart,'}, {'timestamp': (8871.29, 8873.17), 'text': ' you are shaking my heart.'}, {'timestamp': (8873.17, 8876.85), 'text': ' And the top two words are pizza and confidence.'}, {'timestamp': (8876.85, 8884.99), 'text': " Pizza somehow has a higher probability because maybe the model has never seen this song before so it doesn't know"}, {'timestamp': (8884.99, 8887.99), 'text': ' that the next word is confidence.'}, {'timestamp': (8887.99, 8890.55), 'text': ' So maybe the model outputs these probabilities.'}, {'timestamp': (8890.55, 8897.8), 'text': ' But we choose the two top most, the two tokens with the highest probabilities.'}, {'timestamp': (8897.8, 8902.44), 'text': ' At the next time step, we make two prompts.'}, {'timestamp': (8902.44, 8907.24), 'text': ' One in case we choose the first one, so the first token, and one in case we choose the'}, {'timestamp': (8907.24, 8911.97), 'text': ' second token, and then we see what are the next possible choices if we use the first'}, {'timestamp': (8911.97, 8917.41), 'text': ' token and what are the next choices if we use the second token. So we check the model output for'}, {'timestamp': (8917.41, 8927.03), 'text': ' the first prompt and for the second prompt and in case we use for example the first prompt the model will output these probabilities'}, {'timestamp': (8927.03, 8932.03), 'text': ' and if we use the second prompt the model will output these probabilities.'}, {'timestamp': (8932.03, 8937.56), 'text': " What we do then is we calculate the cumulative score for each possible path. So for pizza for example the probability will be 0. è calcolare l'accumulativo scuola per ogni potente."}, {'timestamp': (8937.56, 8941.44), 'text': ' Per pizza, per esempio, la probabilità era 40%,'}, {'timestamp': (8941.44, 8947.84), 'text': ' ma dopo pizza, il modello ha prodotto la probabilità per la margherita, per esempio, è 0,04%.'}, {'timestamp': (8947.84, 8954.69), 'text': " Per questa pazzetta, la margherita è's 0.004 the probability is 0.4%."}, {'timestamp': (8955.81, 8961.89), 'text': " pizza anchovies it's gonna be 0.2% or 0.002."}, {'timestamp': (8963.17, 8970.55), 'text': ' However with confidence we get a new next token that can be either daily or monthly.'}, {'timestamp': (8970.55, 8975.91), 'text': ' With daily we get a cumulative score of 0.16 and with monthly of 0.02.'}, {'timestamp': (8975.91, 8981.84), 'text': ' So as we can see, at the time step 2, even if at the time step 1, pizza was the most'}, {'timestamp': (8981.84, 8987.68), 'text': " probable word, because we kept the second choice alive so we didn't kill it, just like"}, {'timestamp': (8987.68, 8989.68), 'text': ' we did it with greedy.'}, {'timestamp': (8989.68, 8992.53), 'text': ' Let me use the laser. We can see that. kill it, just like we did it with greedy.'}, {'timestamp': (8992.53, 8997.81), 'text': ' We can see that the confidence then produces a next token that is very probable, because'}, {'timestamp': (8997.81, 9006.39), 'text': ' now the model has more prompt and so it can come up with more specific choices for the next tokens.'}, {'timestamp': (9006.39, 9008.07), 'text': ' With a very high confidence.'}, {'timestamp': (9008.07, 9015.55), 'text': ' So we can compute the cumulative score of all these parts and we keep the two parts'}, {'timestamp': (9015.55, 9017.12), 'text': ' that have the top choices.'}, {'timestamp': (9017.12, 9022.56), 'text': " So now the pizza part has been killed because it's later we chose pizza at the beginning"}, {'timestamp': (9022.56, 9026.96), 'text': " because somehow the model thought it was pizza, but then it couldn't find the model was not"}, {'timestamp': (9026.96, 9029.52), 'text': ' so confident about the next words.'}, {'timestamp': (9029.52, 9031.61), 'text': ' But in the case of this token here, the model was very confident about the next words. But in the case of this token here,'}, {'timestamp': (9031.61, 9033.77), 'text': ' the model was very confident about the second score.'}, {'timestamp': (9033.77, 9037.21), 'text': ' So we killed all this path here and we kept this one'}, {'timestamp': (9037.21, 9039.17), 'text': ' until we arrived to the last token'}, {'timestamp': (9039.17, 9042.41), 'text': ' in which we just selected the path with the highest score'}, {'timestamp': (9042.41, 9045.15), 'text': " and that's the output of our inferencing strategy"}, {'timestamp': (9045.15, 9047.87), 'text': ' with Beam Search.'}, {'timestamp': (9047.87, 9053.11), 'text': ' And repeat the steps of the last slide for all the successive tokens until we arrive'}, {'timestamp': (9053.11, 9061.54), 'text': ' to the last one. And with the Beam Search at every step we keep alive the top k parts and all'}, {'timestamp': (9061.54, 9066.84), 'text': ' the others are killed. It increases inferencing time because at every step we must explore'}, {'timestamp': (9066.84, 9071.79), 'text': ' k possible options but generally it performs better than the greedy strategy, for the reason'}, {'timestamp': (9071.79, 9074.69), 'text': ' that I have just shown.'}, {'timestamp': (9074.69, 9078.73), 'text': ' Another thing that is interesting in inferencing is the temperature.'}, {'timestamp': (9078.73, 9085.63), 'text': ' Because the idea of the temperature is that we want to make the we can make the model more confident or less'}, {'timestamp': (9085.63, 9090.83), 'text': ' confident so for example when we can compute the logits which are not the'}, {'timestamp': (9090.83, 9095.47), 'text': ' probabilities so they are what will become the probabilities after we apply'}, {'timestamp': (9095.47, 9098.66), 'text': ' the softmax so before we apply the softmax. So before we apply the softmax, we'}, {'timestamp': (9098.66, 9104.94), 'text': ' can scale the logits so that if we use, for example, like this, so for example, they have'}, {'timestamp': (9104.94, 9110.7), 'text': ' these logits here, I choose the negative numbers, so the softmax probabilities are reasonable numbers.'}, {'timestamp': (9112.7, 9119.26), 'text': ' So these are the logits and if we divide these logits before applying the softmax by a number'}, {'timestamp': (9119.26, 9123.26), 'text': " that is low, so low temperature, it's called, this number is called the temperature,"}, {'timestamp': (9125.63, 9131.79), 'text': ' it will make the model more confident because it will make bigger probabilities bigger and smaller probabilities smaller so the'}, {'timestamp': (9131.79, 9142.4), 'text': ' gap between the low and high probability increases so for example you can see that with without applying any temperature the high probability increases. So for example, you can see that without applying any temperature, the highest logic gets 80%'}, {'timestamp': (9142.4, 9143.4), 'text': ' probability.'}, {'timestamp': (9143.4, 9149.74), 'text': ' But applying a 0.4 temperature, the highest logic becomes 98% probability.'}, {'timestamp': (9149.74, 9154.14), 'text': ' And if we apply a high temperature, it makes the model less confident, so the gap between'}, {'timestamp': (9154.14, 9158.14), 'text': ' the low and high probability reduces.'}, {'timestamp': (9158.14, 9162.98), 'text': ' The temperature is important if we want to increase the confidence of the model or not'}, {'timestamp': (9162.98, 9165.25), 'text': ' because it can be used in conjunction'}, {'timestamp': (9165.25, 9170.11), 'text': ' with other strategies like for example the greedy or the topk or the topv that we will'}, {'timestamp': (9170.11, 9173.61), 'text': ' see later.'}, {'timestamp': (9173.61, 9175.97), 'text': ' Another strategy is the random sampling.'}, {'timestamp': (9175.97, 9180.6), 'text': ' So as we saw the logits are not a probability distribution, but after we apply the soft'}, {'timestamp': (9180.6, 9183.04), 'text': ' max they become a distribution.'}, {'timestamp': (9183.04, 9188.08), 'text': " So what we do, because it's a distribution, we can also sample from this distribution."}, {'timestamp': (9188.08, 9193.34), 'text': ' For example, in this distribution here, that comes from these logits here, we have one'}, {'timestamp': (9193.34, 9198.82), 'text': ' token that can be chosen with 12% probability, one can be chosen with 7% probability and'}, {'timestamp': (9198.82, 9201.42), 'text': ' one that can be chosen with 80% probability.'}, {'timestamp': (9201.42, 9206.91), 'text': ' If we flip a coin, by 80% of the time we will choose this token and 12% of the time we will'}, {'timestamp': (9206.91, 9209.75), 'text': ' choose this token and 7% of the time we will choose this token.'}, {'timestamp': (9209.75, 9212.91), 'text': ' So this means sample from this distribution.'}, {'timestamp': (9212.91, 9220.36), 'text': ' It means sample, take a number from this distribution according to its weight, to its probability.'}, {'timestamp': (9220.36, 9227.6), 'text': ' Now there is a problem with this sampling strategy here that with very little probability'}, {'timestamp': (9227.6, 9231.26), 'text': ' it may happen that we choose tokens that are total crap.'}, {'timestamp': (9231.26, 9244.91), 'text': ' For example, in this scenario here, with the greedy strategy or with Dreamsearch, for example, this token here, if we use a random sampling we will choose the word'}, {'timestamp': (9244.91, 9251.31), 'text': ' pizza with 40% probability, the word confidence with 20% probability but with very little'}, {'timestamp': (9251.31, 9257.12), 'text': ' probability it may happen that we will choose the word pokemon with 10% probability. Of course the probability is low so the probability of pokemon with 10% probability.'}, {'timestamp': (9257.12, 9263.08), 'text': ' Of course the probability is low so the probability of us making bad choice is low but there is'}, {'timestamp': (9263.08, 9264.08), 'text': ' this probability.'}, {'timestamp': (9264.08, 9268.4), 'text': ' So this is the problem with random sampling.'}, {'timestamp': (9268.4, 9274.58), 'text': ' The next strategy is top K. In top K what we do is to avoid selecting the crappy tokens'}, {'timestamp': (9274.58, 9276.02), 'text': ' we just remove them.'}, {'timestamp': (9276.02, 9282.86), 'text': " So we take all the logits, we sort them and then we just keep the highest K. So that's"}, {'timestamp': (9282.86, 9288.71), 'text': " the crappy one we just keep the highest k. So that's the crappy one, we just remove them from this distribution."}, {'timestamp': (9288.71, 9291.55), 'text': ' And then we calculate the distribution for the rest.'}, {'timestamp': (9291.55, 9299.44), 'text': ' So we apply the softmax only to the one that survives. The problem is also here, given the following'}, {'timestamp': (9299.44, 9304.8), 'text': ' these two distributions, the low probability tokens can still make their way into the top'}, {'timestamp': (9304.8, 9312.62), 'text': ' K, because it all depends on the distribution to which we apply the top K. Let me give you a graphical example. Imagine'}, {'timestamp': (9312.62, 9329.03), 'text': ' we have a distribution that is very flat. Suppose this distribution here, so some words, this is our vocabulary, this is the probability of each word, so the'}, {'timestamp': (9329.03, 9333.67), 'text': ' word number one, word number two, word number three, word number four, etc.'}, {'timestamp': (9333.67, 9336.8), 'text': ' But more or less all the words have the same probability.'}, {'timestamp': (9336.8, 9342.0), 'text': ' So imagine we take the top 10 words, so it will select all these tokens, right?'}, {'timestamp': (9342.8, 9346.16), 'text': ' Okay, so it will select the token number one, token number two, token number three,'}, {'timestamp': (9346.16, 9349.04), 'text': ' token number four, up to whatever token here is here.'}, {'timestamp': (9350.0, 9352.7), 'text': ' Imagine we have another distribution that is made like'}, {'timestamp': (9352.7, 9359.58), 'text': ' this. So we still have a vocabulary. Vocabulary. We still have a probability distribution. And'}, {'timestamp': (9361.26, 9371.11), 'text': " the distribution is made like this. So because it's sorted, we have a distribution that is very skewed."}, {'timestamp': (9371.11, 9375.95), 'text': ' Because we still keep the top 10, as you can see we will select this token, this token,'}, {'timestamp': (9375.95, 9377.52), 'text': ' this token, this token, this token, this token, this token.'}, {'timestamp': (9377.52, 9382.12), 'text': ' But these tokens here are very crappy compared to this one here.'}, {'timestamp': (9382.12, 9385.9), 'text': ' So they will still make their way into our selection.'}, {'timestamp': (9385.9, 9387.56), 'text': ' And this is not something that we want.'}, {'timestamp': (9387.56, 9390.0), 'text': ' We want to avoid selecting crappy tokens.'}, {'timestamp': (9390.0, 9391.26), 'text': ' But we still want to have some'}, {'timestamp': (9391.26, 9396.74), 'text': " randomness. So we don't want to be totally greedy because sometimes the tokens that are"}, {'timestamp': (9396.74, 9405.83), 'text': " in the top end, maybe they are all reasonable. Also sometimes the prompt can be quite ambiguous so we don't"}, {'timestamp': (9405.83, 9410.87), 'text': ' know which or even as humans we may not know what is the next word to be chosen. So we'}, {'timestamp': (9410.87, 9415.63), 'text': " want some randomness but we also don't want the very low probability tokens. But with"}, {'timestamp': (9415.63, 9418.4), 'text': ' this top case strategy the top probability the low probability tokens, but with this top case strategy the top probability the low'}, {'timestamp': (9418.4, 9422.78), 'text': ' probability tokens can still make their way into our selection.'}, {'timestamp': (9422.78, 9432.82), 'text': ' And this problem is solved with top P. With top P we only keep the tokens with the highest probability such that the cumulative probability is greater'}, {'timestamp': (9432.82, 9436.02), 'text': ' than or equal to the parameter P.'}, {'timestamp': (9436.02, 9437.78), 'text': ' What does this mean?'}, {'timestamp': (9437.78, 9450.03), 'text': ' It means that if we have the previous distributions, so one that is quite flat for example and one that has a mode. So for example'}, {'timestamp': (9450.03, 9460.98), 'text': ' this one. So this one is nearly 90% and the other one is 0.000% but this more or less all of them are like 0.2%'}, {'timestamp': (9460.98, 9464.28), 'text': ' and then they go down.'}, {'timestamp': (9464.28, 9475.94), 'text': " In the case Imagine P is equal to let's say 0.5, in this case we will select all the tokens such that the area under the name of the token is 0.5. In this case, we will select all the tokens such that the area under the curve"}, {'timestamp': (9475.94, 9482.74), 'text': ' is equal to 0.5. But here, because this first token is already 0.9, we will actually only'}, {'timestamp': (9482.74, 9484.81), 'text': ' select one token and all the crapping'}, {'timestamp': (9484.81, 9490.29), 'text': ' ones will not be selected because this area under the curve is already 0.9.'}, {'timestamp': (9490.29, 9500.8), 'text': ' And this is the idea behind the top P. So when the distribution is more flat, we select more tokens because it means that we are more'}, {'timestamp': (9500.8, 9507.44), 'text': ' uncertain about which token to choose. But when we have a big mode, we select fewer tokens.'}, {'timestamp': (9514.5, 9519.78), 'text': ' This way we avoid getting the low probability ones. So now that we reviewed all the strategies for selecting the token, we will implement'}, {'timestamp': (9519.78, 9526.03), 'text': ' it and in the case of Lama, also in the official code, they actually implement the top P strategy.'}, {'timestamp': (9526.03, 9531.73), 'text': ' In my case, I think that the Beam Search is a reasonable choice.'}, {'timestamp': (9531.73, 9536.23), 'text': ' So in my another video, maybe I will make how to implement the Beam Search.'}, {'timestamp': (9536.23, 9537.58), 'text': ' But for now, I will implement the top'}, {'timestamp': (9537.58, 9544.76), 'text': " P. So let's go build it. So we implement the method, let's call it text complication, which"}, {'timestamp': (9544.76, 9565.27), 'text': " is the same name that's used in the original code from Lama. Given a temperature that is 0.6 and so 0.6 means that we want to make the model more confident"}, {'timestamp': (9568.77, 9570.77), 'text': ' top P'}, {'timestamp': (9571.55, 9579.12), 'text': ' Means that we want all the tokens such that the cumulative probability is at least 0.9, so 90%.'}, {'timestamp': (9596.42, 9611.43), 'text': " Okay, I think here should be lowercase. Okay, so if we didn't specify the max generation length, then we just generate the maximum token swap dot max args just generate"}, {'timestamp': (9611.43, 9621.2), 'text': ' as much token as we can up to the sequence length and then we first of all convert each token of the prompt.'}, {'timestamp': (9621.2, 9654.19), 'text': ' So each prompt actually in two tokens using the tokenizer. As we saw before, we need to add the beginning of sentence when we pass the input'}, {'timestamp': (9654.19, 9680.34), 'text': ' to the model for inferencing, but not the end of sentence. Because we specify the MUX batch also for the model when we build it for the KVcache,'}, {'timestamp': (9680.34, 9742.6), 'text': " so we need to make sure that the BAT size of the prompts is not too large. And then max prompt length is the maximum prompt length that we have in the prompt. Eu não estou tentando ler qualquer mensagem, mesmo se você dever, mas ok, por isso é I'm not writing any message even if you should but okay for us it's just a basic debugging."}, {'timestamp': (9742.6, 9820.56), 'text': ' Then the total length is how many tokens we want to get from the model Ok, now we create the list that will contain the generated token. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� This means create a tensor of shape batch size by total length in which each item is actually the padding'}, {'timestamp': (9820.56, 9896.47), 'text': ' token and then we fill the initial tokens with the prompt tokens. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� We also need this variable that tells if we read the end of sentence in any of the prompts. This indicates if the token in this position is a padding token or not, so true.'}, {'timestamp': (9896.47, 9942.52), 'text': ' If the token is a prompt, token, false. token or not, so true. Let me check TQGM range from 1 to 10. ok now we generate one token at a time so the logits come from the model, so set.model.forward.'}, {'timestamp': (9942.52, 9946.08), 'text': ' We need to pass one token at a time.'}, {'timestamp': (9946.08, 9957.54), 'text': " So which token, the one currently we want to output so current minus 1, pause, it's only one token and we"}, {'timestamp': (9957.54, 9992.3), 'text': ' also tell the model what is the position of this token because for the KV cache. And if we use to inference, we always select the last token but because we'}, {'timestamp': (9992.3, 10009.19), 'text': ' are using the carry catch actually our model will only output one token at a time so the next token will be selected according to our topp strategy.'}, {'timestamp': (10009.19, 10010.19), 'text': ' So we have the probabilities.'}, {'timestamp': (10010.19, 10013.51), 'text': ' Now we apply the topp.'}, {'timestamp': (10013.51, 10014.91), 'text': ' I just define it here.'}, {'timestamp': (10014.91, 10018.64), 'text': ' So sample topp and then we implement it.'}, {'timestamp': (10022.24, 10067.32), 'text': " If we didn't specify any temperature, we just use the greedy. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� Okay, now we have the next token according to this strategy or this degree D."}, {'timestamp': (10067.32, 10073.7), 'text': ' Then we only replace the token if it is a padding token.'}, {'timestamp': (10073.7, 10078.7), 'text': ' So the problem is we already have some tokens that come from the prompt.'}, {'timestamp': (10078.7, 10082.7), 'text': ' But we still need to give the prompt to the model.'}, {'timestamp': (10082.7, 10084.99), 'text': ' But we are only giving one token'}, {'timestamp': (10084.99, 10089.75), 'text': ' at a time to the model to build the initial cache, so we will give the first prompt tokens'}, {'timestamp': (10089.75, 10094.23), 'text': ' will be given to the model, not because we care about what the model will output for'}, {'timestamp': (10094.23, 10100.92), 'text': ' those tokens, but only because we want the KVcache to be built for those'}, {'timestamp': (10100.92, 10102.24), 'text': ' positions.'}, {'timestamp': (10102.24, 10108.68), 'text': ' And after we give the last token of the prompt, then we care about what is the model outputting.'}, {'timestamp': (10108.68, 10132.01), 'text': ' So, only replace the next token if it is a padding token. what is the model outputting. tokens full of paddings but then we replace the prompt tokens with the prompt tokens for'}, {'timestamp': (10132.01, 10133.25), 'text': ' the initial tokens.'}, {'timestamp': (10133.25, 10164.19), 'text': ' All the others have to be inferred by the model. This means basically check this mask.'}, {'timestamp': (10164.19, 10165.19), 'text': ' What is this mask?'}, {'timestamp': (10165.19, 10167.07), 'text': " If it's true, if the token is a prompt token."}, {'timestamp': (10167.07, 10171.11), 'text': ' So if it is a prompt token, replace it with this one.'}, {'timestamp': (10171.11, 10203.06), 'text': " And if it's not a prompt token, just keep it the current one. Since we do not care about what the model outputs for the initial prompt tokens but only for"}, {'timestamp': (10203.06, 10207.07), 'text': " the last prompt token, we don't care if we find an"}, {'timestamp': (10207.07, 10212.67), 'text': ' end of sentence position for those tokens. So end of sentence is only reached if we find it for'}, {'timestamp': (10212.67, 10216.27), 'text': ' one of the tokens that we actually want to inference, not the one that we sent to the'}, {'timestamp': (10216.27, 10268.25), 'text': ' model just to build a KVcache. ʕ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ ʔ Okay, this basically means the end of sentence for a particular prompt is reached only if it'}, {'timestamp': (10268.25, 10278.22), 'text': ' was a padding token, so it was not a prompt token, this means not, and we'}, {'timestamp': (10278.22, 10283.71), 'text': ' actually found an NANDOS token from the model output.'}, {'timestamp': (10283.71, 10288.19), 'text': ' If all of the prompts have reached the end of a sentence token, then we stop this for'}, {'timestamp': (10288.19, 10289.19), 'text': ' loop.'}, {'timestamp': (10289.19, 10290.83), 'text': " We don't need to inference anymore."}, {'timestamp': (10290.83, 10357.78), 'text': ' Now we prepare the output. සිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිවිව� This means that if we found the end of sentence token for one of the prompts, we just cut'}, {'timestamp': (10357.78, 10359.78), 'text': ' the prompt output there.'}, {'timestamp': (10359.78, 10362.42), 'text': ' The model output at that particular token.'}, {'timestamp': (10362.42, 10389.85), 'text': " We don't care about what is output next. This is the output text and then we output the tokens and the text."}, {'timestamp': (10389.85, 10393.9), 'text': " Hopefully I didn't make too many typos and mistakes."}, {'timestamp': (10393.9, 10400.42), 'text': ' So now we need to build the sample top P. So we have the logits that are the output of'}, {'timestamp': (10400.42, 10401.42), 'text': ' the model.'}, {'timestamp': (10401.42, 10404.99), 'text': ' We transform them into probabilities by using the softmax, but given'}, {'timestamp': (10404.99, 10410.51), 'text': ' these probabilities we need to use the sample topp strategy to select all the tokens such'}, {'timestamp': (10410.51, 10420.37), 'text': ' that their cumulative probability is equal to topp, which in our case is 0.9 so 90%'}, {'timestamp': (10420.37, 10426.37), 'text': ' and that simple P'}, {'timestamp': (10426.37, 10468.05), 'text': " okay the first thing we do is we sort these probabilities in descending order. כולת Then we create the mask that says which tokens we want to keep and which one we don't want"}, {'timestamp': (10468.05, 10469.05), 'text': ' to keep.'}, {'timestamp': (10469.05, 10475.82), 'text': ' So mask is equal to probability sum minus probability sort.'}, {'timestamp': (10475.82, 10476.66), 'text': ' More than p.'}, {'timestamp': (10476.66, 10479.54), 'text': ' Why do we do minus probability sort?'}, {'timestamp': (10479.54, 10481.94), 'text': ' Because we want to shift.'}, {'timestamp': (10481.94, 10500.17), 'text': ' Let me show you on the slides. want to shift. The probabilities are this one 0.44%, 40%, 6%, 4%, and 3%.'}, {'timestamp': (10500.17, 10501.57), 'text': ' Then we calculated the cumulative.'}, {'timestamp': (10501.57, 10505.41), 'text': " That means up to here it's 44%."}, {'timestamp': (10505.41, 10507.85), 'text': ' Then this one plus this one is 85%.'}, {'timestamp': (10507.85, 10512.42), 'text': ' Then this one plus this one plus this one is 91%. This one plus this one plus this one plus this one is 91%, this one plus this one plus this'}, {'timestamp': (10512.42, 10515.7), 'text': ' one is 96%, etc.'}, {'timestamp': (10515.7, 10530.57), 'text': ' But imagine we have a 0.90% probability or 0.5% probability, we need to keep up to this token here because this one'}, {'timestamp': (10530.57, 10532.03), 'text': " is not enough, it's 0."}, {'timestamp': (10532.03, 10539.73), 'text': " So we need to up to this one, so the first number that is less than or equal to P and it's this in case it's this one. So the first number that is less than or equal to P and it's this in case it's this"}, {'timestamp': (10539.73, 10561.98), 'text': " one. So that's why we shift it. We want also this token inclusive. And this is why we do this minus probability sort."}, {'timestamp': (10561.98, 10567.35), 'text': ' And then we redistribute the probabilities because of course if we remove some items from here'}, {'timestamp': (10567.35, 10571.05), 'text': " They don't sum up to one anymore. So we need to redistribute the probabilities"}, {'timestamp': (10572.15, 10602.54), 'text': ' And this is very easy. OK, then the next token is basically, suppose we keep the first two tokens and then what'}, {'timestamp': (10602.54, 10606.59), 'text': ' we do is we want to sample from them them so the first token is 0.44%'}, {'timestamp': (10607.07, 10613.23), 'text': ' probability the second token is 0.40% probability but after we redistribute their probabilities'}, {'timestamp': (10613.23, 10620.37), 'text': ' actually this one will be a little higher and this one will be a little higher than 40%.'}, {'timestamp': (10620.37, 10624.51), 'text': ' Then we sample. It means that the first token will have a slightly better chances of being'}, {'timestamp': (10624.51, 10648.35), 'text': ' chosen and the second token will have slightly less chance of being selected. sample because we want one token and this is not the next token and then next token because'}, {'timestamp': (10648.35, 10656.97), 'text': ' this indicates which index to select then we need to map that index to the actual number in the vocabulary.'}, {'timestamp': (10656.97, 10663.61), 'text': ' But because we already changed the order of these numbers, because we sorted it, so initially'}, {'timestamp': (10663.61, 10668.49), 'text': ' the logits were built in such a way that the first logit corresponded to the first number'}, {'timestamp': (10668.49, 10669.53), 'text': ' of the vocabulary.'}, {'timestamp': (10669.53, 10675.6), 'text': ' The second logit corresponded to the first number of the vocabulary, but because we sorted it by descending'}, {'timestamp': (10675.6, 10677.38), 'text': ' order this order has been gone.'}, {'timestamp': (10677.38, 10682.72), 'text': " So we don't know now just given the token selected we don't know which number it maps"}, {'timestamp': (10682.72, 10685.07), 'text': ' back into the vocabulary.'}, {'timestamp': (10685.07, 10687.75), 'text': " That's why the sort method returns two arguments."}, {'timestamp': (10687.75, 10691.51), 'text': ' One is the sorted numbers and one is the indexes that it changes.'}, {'timestamp': (10691.51, 10699.69), 'text': ' So it will tell you for each position what was the original position, item in that position? So this is why we actually query using gather'}, {'timestamp': (10700.77, 10703.21), 'text': ' gather allow us to retrieve'}, {'timestamp': (10704.41, 10706.17), 'text': ' from an'}, {'timestamp': (10706.17, 10708.41), 'text': ' element what was the original one?'}, {'timestamp': (10709.49, 10713.58), 'text': ' Given this index here and then we return the next token'}, {'timestamp': (10713.58, 10724.39), 'text': ' and this will map back into their vocabulary directly and this should be it.'}, {'timestamp': (10724.39, 10727.87), 'text': " So now let's create some prompts and let's run the code."}, {'timestamp': (10727.87, 10730.95), 'text': ' I have some prompts here that I copied and pasted.'}, {'timestamp': (10730.95, 10735.03), 'text': " So now let's build the inference code."}, {'timestamp': (10735.03, 10740.37), 'text': ' So out tokens, outText.'}, {'timestamp': (10740.37, 10750.22), 'text': ' We want to generate maximum 64 tokens.'}, {'timestamp': (10750.22, 10757.86), 'text': ' We assert that the LAN of the output text is actually equal to LAN of prompts.'}, {'timestamp': (10757.86, 10758.86), 'text': ' It should be.'}, {'timestamp': (10758.86, 10761.46), 'text': ' So, for I in range.'}, {'timestamp': (10761.46, 10783.37), 'text': ' Hopefully, the model will work. And then we print the output text for each prompt.'}, {'timestamp': (10783.37, 10789.37), 'text': " So let's run the code and let's hope for the best."}, {'timestamp': (10789.37, 10793.7), 'text': ' Okay, self-attention is missing the required forward function.'}, {'timestamp': (10793.7, 10797.7), 'text': " Let's see why."}, {'timestamp': (10809.31, 10813.63), 'text': " Oops, it's forward, it should be forward. Let's run again."}, {'timestamp': (10813.63, 10816.31), 'text': ' Some received.'}, {'timestamp': (10816.31, 10826.81), 'text': " This is wrong because it should be dimension not div but should be dim let's run again"}, {'timestamp': (10847.03, 10866.29), 'text': " for bfloat 16 so let's see why eos token let me check okay now it's training, I just changed this tensor from capital T to small t, I will investigate"}, {'timestamp': (10866.29, 10867.29), 'text': ' why.'}, {'timestamp': (10867.29, 10872.3), 'text': " Well, we have an output, so let's check."}, {'timestamp': (10872.3, 10875.02), 'text': " First of all, let's check the prompt."}, {'timestamp': (10875.02, 10882.5), 'text': ' Simply put, the theory of relativity states that time is relative to the observer, mass'}, {'timestamp': (10882.5, 10887.03), 'text': ' is relative to the observer, speed is relative to the observer, energy is relative to the observer, and the value of time is relative to the observer, mass is relative to the observer, speed is relative to the observer, energy is relative to the...'}, {'timestamp': (10887.03, 10890.03), 'text': " So, it looks like it's not bad."}, {'timestamp': (10890.03, 10895.03), 'text': ' Suppose the second problem is if Google was an Italian company founded in Milan, it would...'}, {'timestamp': (10895.03, 10900.73), 'text': ' It would be listed on the Milanenit Stock Exchange as the Minenit Stock Exchange is the largest'}, {'timestamp': (10900.73, 10905.05), 'text': ' in Italy, but since Google is a US company, it is listed on the Nasdaq Stock Exchange,'}, {'timestamp': (10905.05, 10907.77), 'text': ' so it avoided actually answering the question.'}, {'timestamp': (10907.77, 10909.17), 'text': " Let's try the few-shot prompt."}, {'timestamp': (10909.17, 10912.54), 'text': ' So this is how you copy it actually from the Lama code.'}, {'timestamp': (10912.54, 10918.58), 'text': ' So they asked to translate from English to French and after cheese, we expect to find'}, {'timestamp': (10918.58, 10921.5), 'text': ' fromage, onion, onion, etc.'}, {'timestamp': (10921.5, 10923.1), 'text': ' So it looks correct.'}, {'timestamp': (10923.1, 10940.29), 'text': " And we can also see that the spaces have been kept. and And then I created the zero shot prompt so tell me if the following person is actually the rhyme on this guy is that's human"}, {'timestamp': (10940.29, 10942.29), 'text': ' So the name is Omar Jamil and the decision is'}, {'timestamp': (10943.21, 10949.25), 'text': " He's a hero in every sense of the world. He's a hero in every sense of the world. I'm very happy Lama"}, {'timestamp': (10954.02, 10959.74), 'text': ' Actually, okay, this is the output of the model with the manual seat zero. If I think I changed the seat to something other number and the model again, the output'}, {'timestamp': (10959.74, 10962.5), 'text': ' will be totally different or maybe slightly different.'}, {'timestamp': (10962.5, 10965.55), 'text': ' I hope not, but it may be different. Anyway,'}, {'timestamp': (10965.55, 10973.39), 'text': ' thanks for watching my video guys. I tried to convey the idea of what made, what is the architecture'}, {'timestamp': (10973.39, 10978.33), 'text': " inside Lama and even if I didn't build the training code because actually to build the"}, {'timestamp': (10978.33, 10985.53), 'text': ' training code is rather complicated. We need a big corpus of text, we need to tokenize it and'}, {'timestamp': (10985.53, 10993.66), 'text': " it's gonna take a long time. But I hope to make another video in the future on how to train a language model,"}, {'timestamp': (10993.66, 11000.14), 'text': ' maybe with a smaller dataset and with a lighter architecture. And I tried to convey all the math'}, {'timestamp': (11000.14, 11008.03), 'text': ' behind all the choices and also how the inner workings of the KVcache and the grouped query'}, {'timestamp': (11008.03, 11012.99), 'text': ' attention. If you have any questions, please write in the comments. I also will share the'}, {'timestamp': (11012.99, 11019.21), 'text': ' repository with the code that I have previously built for this, which has much more comments'}, {'timestamp': (11019.21, 11021.41), 'text': ' than the one I have written here.'}, {'timestamp': (11021.41, 11025.81), 'text': " It's much more in detail, so everyone can understand step by step all the dimensions"}, {'timestamp': (11025.81, 11026.81), 'text': ' involved.'}, {'timestamp': (11026.81, 11031.7), 'text': " Here I tried to write the most important dimensions, but because of time I didn't write all of them."}, {'timestamp': (11033.2, 11039.2), 'text': ' So thank you again guys for watching, it was a long journey but I can assure you that you learned a lot hopefully.'}, {'timestamp': (11039.2, 11045.87), 'text': ' And I hope you will visit again my channel for more videos about deep learning about PyTorch, about coding and about'}, {'timestamp': (11046.59, 11050.67), 'text': ' everything that we love in AI. Thank you for watching guys, have a nice day!'}]